import gradio as gr
import yt_dlp
import os
import json
import base64
import io
import torch
import time
import subprocess
import whisper
import re
import cv2
import numpy as np
import librosa
from typing import List, Dict, Tuple
from PIL import Image, ImageDraw, ImageFont
import tempfile
from collections import defaultdict, deque
import threading
import queue
import sqlite3
from datetime import datetime
import asyncio

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print("‚úÖ YOLOv11 available for real-time detection!")
except ImportError:
    YOLO_AVAILABLE = False
    print("‚ö†Ô∏è YOLOv11 not available. Install with: pip install ultralytics")

# --- Constants ---
DOWNLOAD_HISTORY_FILE = "download_history.json"
MAX_HISTORY_ITEMS = 50
OUTPUT_FOLDER = "downloads"
CLIPS_FOLDER = "clips"
SAMPLE_RATE = 16000

# --- Setup ---
for folder in [OUTPUT_FOLDER, CLIPS_FOLDER]:
    if not os.path.exists(folder):
        os.makedirs(folder)

# --- History Management & YouTube Functions (Unchanged) ---
def load_history(history_file):
    if not os.path.exists(history_file): return []
    try:
        with open(history_file, 'r', encoding='utf-8') as f: return json.load(f)
    except (json.JSONDecodeError, IOError): return []

def save_history(new_entry, history_file):
    history = load_history(history_file)
    history.insert(0, new_entry)
    history = history[:MAX_HISTORY_ITEMS]
    with open(history_file, 'w', encoding='utf-8') as f: json.dump(history, f, indent=4)

def update_history_display(history_data, slider_value, items_per_page=6):
    if not history_data: return gr.Gallery(value=[]), gr.Textbox("No history yet")
    start_idx = max(0, int(slider_value)); end_idx = min(start_idx + items_per_page, len(history_data))
    displayed_items = history_data[start_idx:end_idx]
    gallery_items = [(item.get('thumbnail_url', ''), f"{item.get('title', 'Unknown')[:50]}...") for item in displayed_items]
    info_text = f"Showing items {start_idx + 1}-{end_idx} of {len(history_data)} total"
    return gr.Gallery(value=gallery_items), gr.Textbox(info_text)

def get_video_info(url):
    if not url: return {result_output: gr.Textbox("Please enter a URL first.", visible=True)}
    try:
        with yt_dlp.YoutubeDL({'quiet': True, 'no_warnings': True}) as ydl:
            info = ydl.extract_info(url, download=False)
            return {
                preview_group: gr.Group(visible=True), preview_thumbnail: gr.Image(value=info.get('thumbnail'), label="Thumbnail"),
                preview_title: gr.Textbox(value=info.get('title', 'N/A'), label="Title"),
                preview_duration: gr.Textbox(value=f"{info.get('duration', 0) // 60}:{info.get('duration', 0) % 60:02d}" if info.get('duration') else 'N/A', label="Duration"),
                preview_uploader: gr.Textbox(value=info.get('uploader', 'N/A'), label="Channel"),
                hidden_thumbnail_url: gr.Textbox(value=info.get('thumbnail')), download_button: gr.Button(interactive=True),
                result_output: gr.Textbox("‚ú® Preview loaded! Ready to download.", visible=True)
            }
    except Exception as e:
        return { preview_group: gr.Group(visible=False), download_button: gr.Button(interactive=False), result_output: gr.Textbox(f"‚ùå Error: {str(e)}", visible=True) }

def download_video_or_audio(url, is_audio_only, video_quality, video_format, audio_format, thumbnail_url, use_gpu, progress=gr.Progress(track_tqdm=True)):
    progress(0, desc="Starting...")
    output_template = os.path.join(OUTPUT_FOLDER, '%(title)s - %(id)s.%(ext)s')
    ydl_opts = {'outtmpl': output_template, 'progress_hooks': [lambda d: progress((d.get('downloaded_bytes', 0) / d.get('total_bytes', 1)), desc=f"{d['_percent_str']} of {d['_total_bytes_str']}") if d['status'] == 'downloading' else None]}
    media_type = 'audio' if is_audio_only else 'video'; final_ext = audio_format if is_audio_only else video_format
    
    # GPU Optimization: Get params if GPU is enabled and available
    decode_params = []
    encode_params = []
    if use_gpu and gpu_type:
        decode_params = get_optimized_ffmpeg_params('decode')
        encode_params = get_optimized_ffmpeg_params('encode', 'h264')  # Use H.264 for broad compatibility
    
    if is_audio_only: 
        ydl_opts.update({'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': audio_format}]})
        if use_gpu and gpu_type:
            # Audio extraction is light, but add GPU if possible (e.g., for resampling)
            ydl_opts['postprocessor_args'] = {'ffmpeg': encode_params + ['-c:a', 'aac']}  # Example for audio
    else: 
        # Optimized: More flexible video format selection with GPU-friendly fallbacks
        quality_number = video_quality[:-1]  # Remove 'p' from '720p'
        
        # Prioritize formats that avoid re-encoding (e.g., MP4/H.264 sources)
        format_options = [
            f'bestvideo[height<={quality_number}][ext=mp4]+bestaudio[ext=m4a]/best[height<={quality_number}][ext=mp4]',  # Prefer MP4 to avoid conversion
            f'bestvideo[height<={quality_number}]+bestaudio/best[height<={quality_number}]',
            f'best[height<={quality_number}]',
            f'best[ext={video_format}]',
            'best'
        ]
        
        format_string = '/'.join(format_options)
        ydl_opts.update({
            'format': format_string,
            'merge_output_format': video_format,
        })
        
        # GPU Optimization: If conversion needed, use GPU encoding
        postprocessors = []
        if video_format != 'mp4':
            postprocessors.append({
                'key': 'FFmpegVideoConvertor',
                'preferedformat': video_format,
            })
        
        ydl_opts['postprocessors'] = postprocessors
        
        # Apply GPU accel to FFmpeg postprocessing (merge/convert)
        if use_gpu and gpu_type:
            # Add decode + encode params to FFmpeg args
            ffmpeg_args = decode_params + encode_params + ['-c:a', 'aac']  # Audio to AAC for compatibility
            ydl_opts['postprocessor_args'] = {'ffmpeg': ffmpeg_args}
            print(f"üöÄ GPU acceleration enabled: {gpu_config['name']} with params {ffmpeg_args}")
        else:
            # CPU fallback: Use faster preset to reduce heat/time
            ydl_opts['postprocessor_args'] = {'ffmpeg': ['-c:v', 'libx264', '-preset', 'ultrafast', '-c:a', 'aac']}
            print("‚ö†Ô∏è Using CPU with ultrafast preset")
        
        # Optimization: If no conversion needed, try to copy streams (super fast)
        if 'postprocessors' not in ydl_opts or not ydl_opts['postprocessors']:
            ydl_opts['postprocessor_args'] = {'ffmpeg': ['-c', 'copy']}  # Remux without re-encoding
        
        print(f"üé¨ Using format string: {format_string}")
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            if is_audio_only: info['ext'] = final_ext
            final_filename = ydl.prepare_filename(info)
            
            # Handle cases where the extension might be different (unchanged)
            if not os.path.exists(final_filename): 
                base, _ = os.path.splitext(final_filename)
                possible_extensions = [final_ext, 'mp4', 'webm', 'mkv', 'm4a', 'mp3', 'wav']
                for ext in possible_extensions:
                    test_filename = f"{base}.{ext}"
                    if os.path.exists(test_filename):
                        final_filename = test_filename
                        print(f"‚úÖ Found file with extension: {ext}")
                        break
                else:
                    raise FileNotFoundError(f"Downloaded file not found. Tried: {final_filename}")
        
        # Rest of the function remains unchanged (history saving, updates, etc.)
        history_entry = { 
            "title": info.get('title', 'Unknown Title'), 
            "thumbnail_url": thumbnail_url, 
            "local_filepath": final_filename, 
            "media_type": media_type, 
            "uploader": info.get('uploader', 'Unknown'), 
            "duration": info.get('duration', 0) 
        }
        save_history(history_entry, DOWNLOAD_HISTORY_FILE)
        updated_history = load_history(DOWNLOAD_HISTORY_FILE)
        playback_component = gr.Video if media_type == 'video' else gr.Audio; other_component = gr.Audio if media_type == 'video' else gr.Video
        gallery_update, info_update = update_history_display(updated_history, 0)
        max_slider = max(1, len(updated_history) - 5) if len(updated_history) > 6 else 1
        video_choices = [f"[{item['media_type']}] {item['title']}" for item in updated_history if item['media_type'] == 'video']
        return ( 
            gr.Textbox(f"‚úÖ Success! Downloaded: {info.get('title', 'Unknown')}", visible=True), 
            playback_component(value=final_filename, visible=True), 
            other_component(visible=False), 
            gallery_update, 
            info_update, 
            gr.update(maximum=max_slider, value=0), 
            updated_history, 
            gr.Dropdown(choices=video_choices), 
            gr.Dropdown(choices=video_choices) 
        )
    except Exception as e:
        print(f"‚ùå Download error details: {str(e)}")
        updated_history = load_history(DOWNLOAD_HISTORY_FILE)
        gallery_update, info_update = update_history_display(updated_history, 0)
        max_slider = max(1, len(updated_history) - 5) if len(updated_history) > 6 else 1
        video_choices = [f"[{item['media_type']}] {item['title']}" for item in updated_history if item['media_type'] == 'video']
        return ( 
            gr.Textbox(f"‚ùå Download Error: {str(e)}", visible=True), 
            gr.Video(visible=False), 
            gr.Audio(visible=False), 
            gallery_update, 
            info_update, 
            gr.update(maximum=max_slider, value=0), 
            updated_history, 
            gr.Dropdown(choices=video_choices), 
            gr.Dropdown(choices=video_choices) 
        )
    
def play_from_download_history(history_data, slider_value, evt: gr.SelectData):
    try:
        items_per_page = 6; start_idx = max(0, int(slider_value)); actual_index = start_idx + evt.index
        if actual_index >= len(history_data): return gr.Video(visible=False), gr.Audio(visible=False)
        selected_item = history_data[actual_index]; filepath = selected_item['local_filepath']; media_type = selected_item.get('media_type', 'video')
        if media_type == 'video': return gr.Video(value=filepath, visible=True), gr.Audio(visible=False)
        else: return gr.Video(visible=False), gr.Audio(value=filepath, visible=True)
    except (IndexError, KeyError): return gr.Video(visible=False), gr.Audio(visible=False)

# --- NEW: Vision Analysis Functions ---
def extract_frames_at_intervals(video_path, interval_seconds=5, max_frames=50):
    """Extract frames from video at regular intervals for analysis"""
    print(f"üé¨ Extracting frames from: {video_path}")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("‚ùå Could not open video file")
        return []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    print(f"üìä Video info: {duration:.1f}s, {fps:.1f} FPS, {total_frames} frames")
    
    frames = []
    timestamps = []
    
    # Calculate frame interval
    frame_interval = int(fps * interval_seconds)
    max_frames_to_extract = min(max_frames, int(duration / interval_seconds))
    
    print(f"üîÑ Extracting {max_frames_to_extract} frames at {interval_seconds}s intervals")
    
    for i in range(0, min(total_frames, max_frames_to_extract * frame_interval), frame_interval):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        
        if ret:
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
            timestamps.append(i / fps)
            
    cap.release()
    print(f"‚úÖ Extracted {len(frames)} frames successfully")
    return frames, timestamps

def detect_scene_changes(video_path, threshold=0.3):
    """Detect scene changes in video using frame difference analysis"""
    print(f"üîç Detecting scene changes in: {video_path}")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    scene_changes = [0]  # Start with first frame
    
    # Read first frame
    ret, prev_frame = cap.read()
    if not ret:
        cap.release()
        return scene_changes
    
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    frame_count = 0
    
    print("üé≠ Analyzing frames for scene changes...")
    
    while True:
        ret, curr_frame = cap.read()
        if not ret:
            break
            
        frame_count += 1
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate frame difference
        diff = cv2.absdiff(prev_gray, curr_gray)
        diff_score = np.mean(diff) / 255.0
        
        # If difference is above threshold, it's a scene change
        if diff_score > threshold:
            timestamp = frame_count / fps
            scene_changes.append(timestamp)
            print(f"üé¨ Scene change detected at {timestamp:.1f}s (score: {diff_score:.3f})")
            
        prev_gray = curr_gray
    
    cap.release()
    print(f"‚úÖ Found {len(scene_changes)} scene changes")
    return scene_changes

def analyze_frame_quality(frames, timestamps):
    """Analyze frames for visual quality and suggest best thumbnails"""
    print("üñºÔ∏è Analyzing frame quality for thumbnail suggestions...")
    
    frame_scores = []
    
    for i, frame in enumerate(frames):
        # Convert to grayscale for analysis
        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        
        # Calculate various quality metrics
        # 1. Sharpness (Laplacian variance)
        sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        # 2. Contrast (standard deviation)
        contrast = np.std(gray)
        
        # 3. Brightness (avoid too dark/bright)
        brightness = np.mean(gray)
        brightness_score = 1.0 - abs(brightness - 128) / 128
        
        # 4. Edge density (more edges = more interesting)
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (gray.shape[0] * gray.shape[1])
        
        # Combined score
        total_score = (sharpness * 0.3 + contrast * 0.3 + brightness_score * 0.2 + edge_density * 1000 * 0.2)
        
        frame_scores.append({
            'index': i,
            'timestamp': timestamps[i],
            'score': total_score,
            'sharpness': sharpness,
            'contrast': contrast,
            'brightness': brightness,
            'edge_density': edge_density,
            'frame': frame
        })
        
        print(f"üìä Frame {i} ({timestamps[i]:.1f}s): score={total_score:.1f}")
    
    # Sort by score and return top frames
    frame_scores.sort(key=lambda x: x['score'], reverse=True)
    print(f"‚úÖ Frame quality analysis complete")
    
    return frame_scores

def detect_silence_periods(video_path, silence_threshold=-40, min_silence_duration=1.0):
    """Detect silent periods in audio for smart trimming"""
    print(f"üîá Detecting silence periods in: {video_path}")
    
    try:
        # Extract audio using librosa
        audio, sr = librosa.load(video_path, sr=None)
        print(f"üéµ Loaded audio: {len(audio)/sr:.1f}s at {sr}Hz")
        
        # Convert to dB
        audio_db = librosa.amplitude_to_db(np.abs(audio))
        
        # Find silent periods
        silent_frames = audio_db < silence_threshold
        
        # Convert frame indices to time
        frame_times = librosa.frames_to_time(np.arange(len(silent_frames)), sr=sr)
        
        # Group consecutive silent frames
        silent_periods = []
        in_silence = False
        silence_start = 0
        
        for i, is_silent in enumerate(silent_frames):
            if is_silent and not in_silence:
                # Start of silence
                in_silence = True
                silence_start = frame_times[i]
            elif not is_silent and in_silence:
                # End of silence
                in_silence = False
                silence_duration = frame_times[i] - silence_start
                if silence_duration >= min_silence_duration:
                    silent_periods.append((silence_start, frame_times[i], silence_duration))
                    print(f"üîá Silent period: {silence_start:.1f}s - {frame_times[i]:.1f}s ({silence_duration:.1f}s)")
        
        print(f"‚úÖ Found {len(silent_periods)} silent periods")
        return silent_periods
        
    except Exception as e:
        print(f"‚ùå Error detecting silence: {str(e)}")
        return []

def generate_auto_chapters(video_path, transcript_segments, scene_changes):
    """Generate automatic chapters based on transcript and scene analysis"""
    print("üìö Generating automatic chapters...")
    
    chapters = []
    
    # Start with scene changes as potential chapter points
    potential_chapters = [(0, "Introduction")]
    
    # Add scene changes with smarter titles
    for i, scene_time in enumerate(scene_changes[1:], 1):  # Skip first scene change (0)
        # Find segments around this scene change (¬±10 seconds)
        relevant_segments = [
            seg for seg in transcript_segments 
            if abs(seg['start'] - scene_time) <= 10
        ]
        
        if relevant_segments:
            # Find the segment closest to the scene change
            closest_segment = min(relevant_segments, 
                                key=lambda x: abs(x['start'] - scene_time))
            
            # Extract meaningful words for chapter title
            text = closest_segment['text'].strip().lower()
            
            # Look for key topic indicators
            topic_words = []
            important_words = ['setup', 'install', 'configure', 'build', 'create', 'add', 'remove', 
                             'test', 'demo', 'example', 'tutorial', 'guide', 'step', 'part',
                             'feature', 'function', 'method', 'process', 'system', 'application',
                             'review', 'analysis', 'comparison', 'conclusion', 'summary', 'results',
                             'performance', 'gaming', 'benchmark', 'fps', 'cpu', 'gpu', 'motherboard',
                             'memory', 'storage', 'graphics', 'processor', 'component']
            
            # Find important words in the segment
            words = text.split()
            for word in words:
                clean_word = re.sub(r'[^\w]', '', word)
                if clean_word in important_words:
                    topic_words.append(clean_word.title())
            
            # Create chapter title
            if topic_words:
                chapter_title = f"{' & '.join(topic_words[:2])}"  # Max 2 key words
            else:
                # Fallback: use first meaningful words
                meaningful_words = [w for w in words if len(w) > 3 and w not in ['this', 'that', 'with', 'from', 'have', 'will', 'they', 'were', 'been']]
                if meaningful_words:
                    chapter_title = ' '.join(meaningful_words[:3]).title()
                else:
                    chapter_title = f"Part {i}"
        else:
            chapter_title = f"Scene {i}"
            
        potential_chapters.append((scene_time, chapter_title))
    
    # Sort by timestamp
    potential_chapters.sort(key=lambda x: x[0])
    
    # Format chapters
    for i, (timestamp, title) in enumerate(potential_chapters):
        minutes = int(timestamp // 60)
        seconds = int(timestamp % 60)
        time_str = f"{minutes:02d}:{seconds:02d}"
        
        chapters.append({
            'timestamp': timestamp,
            'time_string': time_str,
            'title': title
        })
        
        print(f"üìñ Chapter {i+1}: {time_str} - {title}")
    
    print(f"‚úÖ Generated {len(chapters)} chapters")
    return chapters

def create_highlight_reel(video_path, transcript_segments, target_duration=60):
    """Create an AI-generated highlight reel"""
    print(f"üé¨ Creating {target_duration}s highlight reel from: {video_path}")
    
    # Score segments based on various factors
    segment_scores = []
    
    for segment in transcript_segments:
        text = segment['text'].lower()
        duration = segment['end'] - segment['start']
        
        # Scoring factors
        score = 0
        
        # Prefer segments with key phrases
        key_phrases = ['important', 'key', 'main', 'crucial', 'best', 'amazing', 'incredible', 'wow']
        for phrase in key_phrases:
            if phrase in text:
                score += 2
        
        # Prefer medium-length segments (not too short, not too long)
        if 3 <= duration <= 8:
            score += 3
        elif 2 <= duration <= 10:
            score += 1
            
        # Prefer segments with excitement (exclamation marks, caps)
        if '!' in segment['text'] or any(word.isupper() for word in segment['text'].split()):
            score += 1
            
        segment_scores.append({
            'segment': segment,
            'score': score,
            'duration': duration
        })
    
    # Sort by score
    segment_scores.sort(key=lambda x: x['score'], reverse=True)
    
    # Select segments until we reach target duration
    selected_segments = []
    total_duration = 0
    
    for item in segment_scores:
        if total_duration + item['duration'] <= target_duration:
            selected_segments.append(item['segment'])
            total_duration += item['duration']
            print(f"‚ûï Selected segment: {item['segment']['start']:.1f}s - {item['segment']['text'][:50]}...")
    
    # Sort selected segments by timestamp
    selected_segments.sort(key=lambda x: x['start'])
    
    return selected_segments

# --- Vision Lab Analysis Functions ---
def analyze_video_comprehensive(video_path_str, history_data, frame_interval, scene_threshold, progress=gr.Progress(track_tqdm=True)):
    """Comprehensive video analysis using AI vision and audio analysis"""
    print("üî¨ Starting comprehensive video analysis")
    print(f"üìπ Video path string: {video_path_str}")
    
    if not video_path_str or video_path_str == "No videos available":
        print("‚ùå No video selected")
        raise gr.Error("Please select a video from the dropdown first.")
    
    selected_title = video_path_str.split("] ", 1)[1]
    print(f"üè∑Ô∏è Selected title: {selected_title}")
    
    video_item = next((item for item in history_data if item['title'] == selected_title), None)
    if not video_item:
        print("‚ùå Video item not found in history")
        raise gr.Error("Could not find the selected video file.")
    
    video_path = video_item['local_filepath']
    print(f"üìÇ Video path: {video_path}")
    
    if not os.path.exists(video_path):
        print("‚ùå Video file not found")
        raise gr.Error(f"Video file not found at path: {video_path}")
    
    try:
        analysis_results = {}
        
        # Step 1: Extract frames for analysis
        progress(0.1, desc="üé¨ Extracting frames for analysis...")
        frames, timestamps = extract_frames_at_intervals(video_path, frame_interval)
        analysis_results['frames'] = frames
        analysis_results['timestamps'] = timestamps
        
        # Step 2: Detect scene changes
        progress(0.3, desc="üé≠ Detecting scene changes...")
        scene_changes = detect_scene_changes(video_path, scene_threshold)
        analysis_results['scene_changes'] = scene_changes
        
        # Step 3: Analyze frame quality for thumbnails
        progress(0.5, desc="üñºÔ∏è Analyzing frame quality...")
        frame_quality = analyze_frame_quality(frames, timestamps)
        analysis_results['frame_quality'] = frame_quality
        
        # Step 4: Detect silence periods
        progress(0.7, desc="üîá Detecting silence periods...")
        silence_periods = detect_silence_periods(video_path)
        analysis_results['silence_periods'] = silence_periods
        
        # Step 5: Store results globally  
        progress(0.9, desc="üíæ Storing analysis results...")
        global current_vision_analysis
        current_vision_analysis = {
            'video_path': video_path,
            'video_title': selected_title,
            'analysis': analysis_results
        }
        
        # Store globally for other functions to access
        globals()['current_vision_analysis'] = current_vision_analysis
        
        progress(1.0, desc="‚úÖ Analysis complete!")
        print("‚úÖ Comprehensive video analysis completed successfully!")
        
        # Generate overview HTML
        overview_html = generate_analysis_overview(analysis_results, selected_title)
        
        # Generate scene and silence analysis HTML with direct data passing
        scene_html = generate_scene_analysis_html_direct(analysis_results, selected_title)
        silence_html = generate_silence_analysis_html_direct(analysis_results, selected_title)
        
        # Generate top thumbnail suggestions
        top_thumbnails = []
        if frame_quality:
            for item in frame_quality[:6]:  # Top 6 thumbnails
                # Convert frame to PIL Image and then to gradio format
                pil_image = Image.fromarray(item['frame'])
                top_thumbnails.append((pil_image, f"Score: {item['score']:.1f} - {item['timestamp']:.1f}s"))
        
        return (
            video_path,  # vision_video_player
            overview_html,  # analysis_overview
            gr.update(value=top_thumbnails),  # thumbnails_gallery - Fixed: use 'value' not 'choices'
            gr.update(interactive=True),  # generate_chapters_btn
            gr.update(interactive=True),  # smart_thumbnails_btn
            gr.update(interactive=True),  # silence_remover_btn
            gr.update(interactive=True),  # highlight_reel_btn
            gr.update(interactive=True),  # split_scenes_btn
            gr.update(interactive=True),  # export_analysis_btn
            "‚úÖ Analysis completed successfully! All smart features are now available.",  # analysis_status
            scene_html,  # scene_timeline
            silence_html  # silence_analysis
        )
        
    except Exception as e:
        print(f"‚ùå Error in video analysis: {str(e)}")
        raise gr.Error(f"Error during video analysis: {str(e)}")

def generate_analysis_overview(analysis_results, video_title):
    """Generate HTML overview of analysis results"""
    frames = analysis_results.get('frames', [])
    scene_changes = analysis_results.get('scene_changes', [])
    silence_periods = analysis_results.get('silence_periods', [])
    frame_quality = analysis_results.get('frame_quality', [])
    
    total_silence = sum(period[2] for period in silence_periods)
    best_frame_score = max(item['score'] for item in frame_quality) if frame_quality else 0
    
    # Get GPU acceleration status
    gpu_status = "GPU Accelerated" if 'gpu_type' in globals() and globals()['gpu_type'] else "CPU Processing"
    gpu_icon = "üöÄ" if 'gpu_type' in globals() and globals()['gpu_type'] else "üñ•Ô∏è"
    
    html = f"""
    <div style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;">
        <h2 style="margin: 0 0 20px 0; color: white;">üìä Analysis Overview: {video_title}</h2>
        
        <div style="background: rgba(255,255,255,0.1); padding: 10px; border-radius: 8px; margin-bottom: 20px; text-align: center;">
            <strong>{gpu_icon} {gpu_status}</strong> - All video processing operations will use optimized acceleration
        </div>
        
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-bottom: 20px;">
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üé¨</div>
                <div style="font-size: 1.5em; font-weight: bold;">{len(frames)}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Frames Analyzed</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üé≠</div>
                <div style="font-size: 1.5em; font-weight: bold;">{len(scene_changes)}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Scene Changes</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üîá</div>
                <div style="font-size: 1.5em; font-weight: bold;">{total_silence:.1f}s</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Total Silence</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üñºÔ∏è</div>
                <div style="font-size: 1.5em; font-weight: bold;">{best_frame_score:.1f}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Best Quality Score</div>
            </div>
            
        </div>
        
        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;">
            <h3 style="margin: 0 0 10px 0; color: white;">üéØ Quick Actions Available:</h3>
            <ul style="margin: 0; padding-left: 20px; color: white;">
                <li>üìö Generate automatic chapters from scene changes</li>
                <li>üñºÔ∏è View AI-suggested thumbnails ranked by quality</li>
                <li>üîá Remove silence to create a fast-paced edit</li>
                <li>‚ú® Create highlight reel with best moments</li>
                <li>‚úÇÔ∏è Split video at detected scene changes</li>
            </ul>
        </div>
    </div>
    """
    
    return html

# --- GPU Acceleration Detection ---
def detect_gpu_acceleration():
    """Detect available GPU acceleration for FFmpeg"""
    print("üîç Detecting available GPU acceleration...")
    
    gpu_options = {
        'nvidia': {
            'decode': ['-hwaccel', 'cuda', '-hwaccel_output_format', 'cuda'],
            'encode_h264': ['-c:v', 'h264_nvenc'],
            'encode_hevc': ['-c:v', 'hevc_nvenc'],
            'name': 'NVIDIA GPU (NVENC)'
        },
        'intel': {
            'decode': ['-hwaccel', 'qsv'],
            'encode_h264': ['-c:v', 'h264_qsv'],
            'encode_hevc': ['-c:v', 'hevc_qsv'],
            'name': 'Intel GPU (Quick Sync)'
        },
        'amd': {
            'decode': ['-hwaccel', 'd3d11va'],
            'encode_h264': ['-c:v', 'h264_amf'],
            'encode_hevc': ['-c:v', 'hevc_amf'],
            'name': 'AMD GPU (AMF)'
        }
    }
    
    # Test which GPU acceleration is available
    for gpu_type, config in gpu_options.items():
        try:
            # Test encode capability
            test_cmd = ['ffmpeg', '-f', 'lavfi', '-i', 'testsrc=duration=1:size=320x240:rate=1'] + config['encode_h264'] + ['-t', '1', '-f', 'null', '-']
            result = subprocess.run(test_cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                print(f"‚úÖ {config['name']} acceleration available")
                return gpu_type, config
                
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            continue
    
    print("‚ö†Ô∏è No GPU acceleration available, using CPU")
    return None, {
        'decode': [],
        'encode_h264': ['-c:v', 'libx264', '-preset', 'fast'],
        'encode_hevc': ['-c:v', 'libx265', '-preset', 'fast'],
        'name': 'CPU (fallback)'
    }

def get_optimized_ffmpeg_params(operation='encode', codec='h264'):
    """Get optimized FFmpeg parameters based on available hardware"""
    global gpu_acceleration_config
    
    if 'gpu_acceleration_config' not in globals():
        gpu_type, gpu_acceleration_config = detect_gpu_acceleration()
        globals()['gpu_acceleration_config'] = gpu_acceleration_config
        globals()['gpu_type'] = gpu_type
    
    config = gpu_acceleration_config
    
    if operation == 'decode':
        return config.get('decode', [])
    elif operation == 'encode':
        if codec == 'hevc':
            return config.get('encode_hevc', config.get('encode_h264', ['-c:v', 'libx264', '-preset', 'fast']))
        else:
            return config.get('encode_h264', ['-c:v', 'libx264', '-preset', 'fast'])
    
    return []

def safe_filename(text, max_length=100):
    """Create a safe filename by removing invalid characters"""
    # Remove or replace invalid characters
    safe_text = re.sub(r'[<>:"/\\|?*]', '_', text)
    # Remove any remaining problematic characters
    safe_text = re.sub(r'[^\w\s\-_.]', '_', safe_text)
    # Replace multiple underscores/spaces with single underscore
    safe_text = re.sub(r'[_\s]+', '_', safe_text)
    # Remove leading/trailing underscores
    safe_text = safe_text.strip('_')
    # Limit length
    if len(safe_text) > max_length:
        safe_text = safe_text[:max_length].strip('_')
    return safe_text if safe_text else "untitled"

def generate_auto_chapters_from_analysis():
    """Generate chapters using stored analysis data"""
    global current_vision_analysis, current_transcript_data
    
    print("üìö Generating auto chapters from analysis...")
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No video analysis available. Please analyze a video first.")
    
    scene_changes = current_vision_analysis['analysis']['scene_changes']
    video_title = current_vision_analysis['video_title']
    
    # Get transcript data if available
    transcript_segments = []
    if 'current_transcript_data' in globals() and current_transcript_data:
        transcript_segments = current_transcript_data.get('segments', [])
    
    if transcript_segments:
        chapters = generate_auto_chapters(current_vision_analysis['video_path'], transcript_segments, scene_changes)
    else:
        # Generate chapters from scene changes only
        chapters = []
        for i, scene_time in enumerate(scene_changes):
            minutes = int(scene_time // 60)
            seconds = int(scene_time % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"
            
            if i == 0:
                title = "Introduction"
            else:
                title = f"Scene {i}"
                
            chapters.append({
                'timestamp': scene_time,
                'time_string': time_str,
                'title': title
            })
    
    # Generate HTML output
    chapters_html = f"""
    <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
        <h3 style="color: #2c3e50; margin-bottom: 20px;">üìö Auto-Generated Chapters for: {video_title}</h3>
        <div style="background: white; border-radius: 8px; padding: 15px;">
    """
    
    for chapter in chapters:
        chapters_html += f"""
        <div style="padding: 10px; border-bottom: 1px solid #eee; cursor: pointer;" 
             onclick="
                try {{
                    let video = document.querySelector('#vision_video_player video');
                    if (video) {{
                        video.currentTime = {chapter['timestamp']};
                        console.log('Jumped to chapter: {chapter['title']} at {chapter['timestamp']}s');
                    }}
                }} catch(e) {{ console.error(e); }}
             ">
            <span style="background: #007bff; color: white; padding: 4px 8px; border-radius: 4px; margin-right: 10px;">
                {chapter['time_string']}
            </span>
            <span style="font-weight: 500; color: #2c3e50;">{chapter['title']}</span>
        </div>
        """
    
    chapters_html += """
        </div>
        <p style="color: #666; font-size: 0.9em; margin-top: 10px;">üí° Click any chapter to jump to that moment in the video!</p>
    </div>
    """
    
    # Create downloadable file
    chapters_text = f"Chapters for: {video_title}\n\n"
    for chapter in chapters:
        chapters_text += f"{chapter['time_string']} - {chapter['title']}\n"
    
    # Save to file
    chapters_filename = f"chapters_{safe_filename(video_title)}.txt"
    chapters_path = os.path.join(OUTPUT_FOLDER, chapters_filename)
    
    with open(chapters_path, 'w', encoding='utf-8') as f:
        f.write(chapters_text)
    
    print(f"‚úÖ Chapters generated and saved to {chapters_filename}")
    
    return (
        chapters_html,
        gr.File(value=chapters_path, visible=True)
    )

def remove_silence_from_video():
    """Remove silent periods from video for fast-paced editing"""
    global current_vision_analysis
    
    print("üîá Starting silence removal...")
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No video analysis available. Please analyze a video first.")
    
    video_path = current_vision_analysis['video_path']
    video_title = current_vision_analysis['video_title']
    silence_periods = current_vision_analysis['analysis']['silence_periods']
    
    if not silence_periods:
        raise gr.Error("No silence periods detected in this video.")
    
    print(f"üé¨ Removing silence from video with {len(silence_periods)} silent periods...")
    
    # Create output filename
    safe_title = safe_filename(video_title)
    output_filename = f"{safe_title}_silence_removed.mp4"
    output_path = os.path.join(CLIPS_FOLDER, output_filename)
    
    try:
        # Use FFmpeg's built-in silenceremove filter with GPU acceleration
        print("üîß Using FFmpeg silenceremove filter with GPU acceleration...")
        
        # Get GPU acceleration parameters
        decode_params = get_optimized_ffmpeg_params('decode')
        encode_params = get_optimized_ffmpeg_params('encode', 'h264')
        
        # Build command with GPU acceleration
        command = ["ffmpeg"]
        
        # Add decode acceleration if available
        if decode_params:
            command.extend(decode_params)
        
        command.extend([
            "-i", video_path,
            "-af", "silenceremove=start_periods=1:start_duration=1:start_threshold=-40dB:detection=peak:stop_periods=-1:stop_duration=1:stop_threshold=-40dB"
        ])
        
        # Add encode acceleration
        command.extend(encode_params)
        
        # Add audio codec and output
        command.extend(["-c:a", "aac", "-y", output_path])
        
        print(f"üöÄ Command: {' '.join(command[:8])}... (GPU accelerated)")
        result = subprocess.run(command, check=True, capture_output=True, text=True)
        
        if os.path.exists(output_path):
            original_size = os.path.getsize(video_path) / (1024*1024)  # MB
            new_size = os.path.getsize(output_path) / (1024*1024)  # MB
            
            # Estimate time saved (approximate based on detected periods)
            estimated_time_saved = sum(period[2] for period in silence_periods[:100])  # Sample estimate
            
            print(f"‚úÖ GPU-accelerated silence removal complete! Estimated time saved: {estimated_time_saved:.1f}s")
            print(f"üìÅ Silence-removed file saved: {output_path}")
            
            return (
                gr.File(value=output_path, visible=True),
                f"‚úÖ Silence removed using GPU acceleration! Estimated time saved: {estimated_time_saved:.1f}s. File: clips/{output_filename} ({new_size:.1f}MB)"
            )
        else:
            raise gr.Error("Failed to create silence-removed video.")
            
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr if e.stderr else str(e)
        print(f"‚ùå GPU FFmpeg error: {error_msg}")
        
        # Fallback: try with CPU and simpler parameters
        print("üîÑ GPU failed, trying CPU fallback...")
        
        try:
            fallback_command = [
                "ffmpeg", "-i", video_path,
                "-af", "silenceremove=stop_periods=-1:stop_duration=2:stop_threshold=-40dB",
                "-c:v", "libx264", "-preset", "fast", "-c:a", "aac",
                "-y", output_path
            ]
            
            subprocess.run(fallback_command, check=True, capture_output=True, text=True)
            
            if os.path.exists(output_path):
                print("‚úÖ CPU fallback silence removal successful!")
                return (
                    gr.File(value=output_path, visible=True),
                    "‚úÖ Silence removed using CPU fallback! Some silent periods may remain for compatibility."
                )
            else:
                raise gr.Error("CPU fallback also failed to create output file.")
                
        except subprocess.CalledProcessError as fallback_e:
            fallback_error = fallback_e.stderr if fallback_e.stderr else str(fallback_e)
            print(f"‚ùå CPU fallback also failed: {fallback_error}")
            raise gr.Error(f"Silence removal failed. FFmpeg error: {fallback_error}")
            
    except Exception as e:
        print(f"‚ùå Unexpected error: {str(e)}")
        raise gr.Error(f"Error removing silence: {str(e)}")

def create_highlight_reel_from_analysis(target_duration):
    """Create highlight reel using stored analysis and transcript data"""
    global current_vision_analysis, current_transcript_data
    
    print(f"‚ú® Creating {target_duration}s highlight reel...")
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No video analysis available. Please analyze a video first.")
    
    if 'current_transcript_data' not in globals() or not current_transcript_data:
        print("‚ùå No transcript data found in globals")
        print(f"üîç Current globals keys: {list(globals().keys())}")
        raise gr.Error("No transcript data available. Please transcribe the video first in Intelligent Scribe tab.")
    
    video_path = current_vision_analysis['video_path']
    video_title = current_vision_analysis['video_title']
    transcript_segments = current_transcript_data.get('segments', [])
    
    print(f"üìä Found transcript with {len(transcript_segments)} segments")
    
    if not transcript_segments:
        raise gr.Error("No transcript segments found. Please transcribe the video first.")
    
    print(f"üìä Analyzing {len(transcript_segments)} transcript segments...")
    
    # Use existing highlight reel logic
    selected_segments = create_highlight_reel(video_path, transcript_segments, target_duration)
    
    if not selected_segments:
        raise gr.Error("No suitable segments found for highlight reel. Try a longer duration or transcribe a more dynamic video.")
    
    print(f"üéØ Selected {len(selected_segments)} segments for highlight reel")
    
    # Create output filename with safe naming
    safe_title = safe_filename(video_title)
    output_filename = f"{safe_title}_highlights_{target_duration}s.mp4"
    output_path = os.path.join(CLIPS_FOLDER, output_filename)
    
    # Create FFmpeg filter to concatenate selected segments
    temp_list_file = os.path.join(CLIPS_FOLDER, f"temp_segments_{int(time.time())}.txt")
    
    try:
        print("üé¨ Creating temporary clips for concatenation...")
        
        # Get GPU acceleration parameters
        decode_params = get_optimized_ffmpeg_params('decode')
        encode_params = get_optimized_ffmpeg_params('encode', 'h264')
        
        # Create temporary clips for each segment
        temp_clips = []
        with open(temp_list_file, 'w') as f:
            for i, segment in enumerate(selected_segments):
                temp_clip = os.path.join(CLIPS_FOLDER, f"temp_clip_{i}_{int(time.time())}.mp4")
                
                # Build GPU-accelerated extraction command
                command = ["ffmpeg"]
                
                # Add decode acceleration if available
                if decode_params:
                    command.extend(decode_params)
                
                command.extend([
                    "-i", video_path,
                    "-ss", str(segment['start']),
                    "-to", str(segment['end'])
                ])
                
                # For short clips, copy is usually faster than re-encoding
                # But if we want to ensure compatibility, we can re-encode
                if segment['end'] - segment['start'] < 10:  # Short clips - copy
                    command.extend(["-c", "copy"])
                else:  # Longer clips - GPU encode for optimization
                    command.extend(encode_params)
                    command.extend(["-c:a", "aac"])
                
                command.extend(["-y", temp_clip])
                
                print(f"  üöÄ Creating clip {i+1}/{len(selected_segments)} with GPU acceleration...")
                subprocess.run(command, check=True, capture_output=True, text=True)
                temp_clips.append(temp_clip)
                f.write(f"file '{os.path.basename(temp_clip)}'\n")
                print(f"  ‚úÖ Created temp clip {i+1}/{len(selected_segments)}")
        
        print("üîó Concatenating clips into highlight reel...")
        
        # Concatenate all clips - use copy for speed since clips should be compatible
        command = [
            "ffmpeg", "-f", "concat", "-safe", "0", "-i", temp_list_file,
            "-c", "copy", "-y", output_path
        ]
        
        subprocess.run(command, check=True, capture_output=True, text=True, cwd=CLIPS_FOLDER)
        
        # Clean up temporary files
        for temp_clip in temp_clips:
            if os.path.exists(temp_clip):
                os.remove(temp_clip)
        if os.path.exists(temp_list_file):
            os.remove(temp_list_file)
        
        if os.path.exists(output_path):
            actual_duration = sum(segment['end'] - segment['start'] for segment in selected_segments)
            file_size = os.path.getsize(output_path) / (1024*1024)  # MB
            
            print(f"‚úÖ Highlight reel created! Duration: {actual_duration:.1f}s, Size: {file_size:.1f}MB")
            print(f"üìÅ Highlight reel saved: {output_path}")
            
            # Generate preview HTML with file location info
            preview_html = f"""
            <div style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;">
                <h3 style="margin: 0 0 15px 0; color: white;">‚ú® Highlight Reel Created Successfully!</h3>
                
                <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; margin-bottom: 15px;">
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px; text-align: center;">
                        <div>
                            <div style="font-size: 1.5em; font-weight: bold;">{actual_duration:.1f}s</div>
                            <div style="font-size: 0.9em; opacity: 0.9;">Duration</div>
                        </div>
                        <div>
                            <div style="font-size: 1.5em; font-weight: bold;">{len(selected_segments)}</div>
                            <div style="font-size: 0.9em; opacity: 0.9;">Segments</div>
                        </div>
                        <div>
                            <div style="font-size: 1.5em; font-weight: bold;">{file_size:.1f}MB</div>
                            <div style="font-size: 0.9em; opacity: 0.9;">File Size</div>
                        </div>
                    </div>
                </div>
                
                <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;">
                    <h4 style="margin-top: 0; color: white;">üìã Included Segments:</h4>
            """
            
            for i, segment in enumerate(selected_segments, 1):
                start_time = time.strftime('%M:%S', time.gmtime(segment['start']))
                preview_html += f"<p style='margin: 5px 0; font-size: 0.9em;'><strong>{i}.</strong> {start_time} - {segment['text'][:60]}...</p>"
            
            preview_html += f"""
                </div>
                
                <div style="background: rgba(46, 204, 113, 0.2); padding: 10px; border-radius: 8px; margin-top: 15px; text-align: center;">
                    <strong>üìÅ File saved to: clips/{output_filename}</strong>
                </div>
            </div>
            """
            
            return (
                preview_html,
                gr.File(value=output_path, visible=True)
            )
        else:
            raise gr.Error("Failed to create highlight reel.")
            
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr if e.stderr else str(e)
        print(f"‚ùå FFmpeg error: {error_msg}")
        raise gr.Error(f"Error creating highlight reel: {error_msg}")
    except Exception as e:
        print(f"‚ùå Unexpected error: {str(e)}")
        raise gr.Error(f"Error creating highlight reel: {str(e)}")
    finally:
        # Always clean up temp files
        try:
            if 'temp_clips' in locals():
                for temp_clip in temp_clips:
                    if os.path.exists(temp_clip):
                        os.remove(temp_clip)
            if 'temp_list_file' in locals() and os.path.exists(temp_list_file):
                os.remove(temp_list_file)
        except:
            pass

def split_video_by_scenes():
    """Split video at detected scene changes"""
    global current_vision_analysis
    
    print("‚úÇÔ∏è Splitting video by scene changes...")
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No video analysis available. Please analyze a video first.")
    
    video_path = current_vision_analysis['video_path']
    video_title = current_vision_analysis['video_title']
    scene_changes = current_vision_analysis['analysis']['scene_changes']
    
    if len(scene_changes) < 2:
        raise gr.Error("Not enough scene changes detected to split video.")
    
    print(f"üé¨ Splitting into {len(scene_changes)} scenes...")
    
    # Create clips for each scene
    safe_title = re.sub(r'[<>:"/\\|?*]', '_', video_title)
    created_files = []
    
    try:
        for i in range(len(scene_changes)):
            start_time = scene_changes[i]
            end_time = scene_changes[i + 1] if i + 1 < len(scene_changes) else None
            
            output_filename = f"{safe_title}_scene_{i+1:02d}.mp4"
            output_path = os.path.join(CLIPS_FOLDER, output_filename)
            
            command = ["ffmpeg", "-i", video_path, "-ss", str(start_time)]
            
            if end_time:
                command.extend(["-to", str(end_time)])
            
            command.extend(["-c", "copy", "-y", output_path])
            
            subprocess.run(command, check=True, capture_output=True, text=True)
            
            if os.path.exists(output_path):
                duration = (end_time - start_time) if end_time else "remainder"
                created_files.append((output_path, f"Scene {i+1}: {duration}s" if isinstance(duration, float) else f"Scene {i+1}: {duration}"))
                print(f"‚úÖ Created scene {i+1}: {output_filename}")
        
        # Create zip file with all scenes
        import zipfile
        zip_filename = f"{safe_title}_all_scenes.zip"
        zip_path = os.path.join(CLIPS_FOLDER, zip_filename)
        
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            for file_path, description in created_files:
                zipf.write(file_path, os.path.basename(file_path))
        
        print(f"‚úÖ Split complete! Created {len(created_files)} scene clips")
        
        return (
            gr.File(value=zip_path, visible=True),
            f"‚úÖ Video split into {len(created_files)} scenes! Download the ZIP file containing all scene clips."
        )
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå FFmpeg error: {e.stderr}")
        raise gr.Error(f"Error splitting video: {e.stderr}")
    except Exception as e:
        print(f"‚ùå Unexpected error: {str(e)}")
        raise gr.Error(f"Error splitting video: {str(e)}")

def generate_scene_analysis_html():
    """Generate HTML visualization of scene analysis"""
    try:
        global current_vision_analysis
        
        if 'current_vision_analysis' not in globals() or not current_vision_analysis:
            return "<p style='color: #666; text-align: center; padding: 40px;'>No analysis data available. Please analyze a video first.</p>"
        
        scene_changes = current_vision_analysis['analysis']['scene_changes']
        video_title = current_vision_analysis['video_title']
        
        if not scene_changes:
            return "<p style='color: #666; text-align: center; padding: 40px;'>No scene changes detected in this video.</p>"
        
        html = f"""
        <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <h3 style="color: #2c3e50; margin-bottom: 20px;">üé≠ Scene Analysis: {video_title}</h3>
            <div style="background: white; border-radius: 8px; padding: 20px;">
                <h4 style="color: #2c3e50;">üìä Scene Timeline</h4>
                <div style="position: relative; height: 60px; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); border-radius: 8px; margin: 20px 0;">
        """
        
        # Calculate total duration for percentage positioning
        total_duration = scene_changes[-1] if scene_changes else 100
        
        for i, scene_time in enumerate(scene_changes):
            percentage = (scene_time / total_duration) * 100
            
            html += f"""
            <div style="position: absolute; left: {percentage}%; top: -5px; width: 2px; height: 70px; background: white; border-radius: 1px;"></div>
            <div style="position: absolute; left: {percentage}%; top: 75px; transform: translateX(-50%); font-size: 0.8em; color: #2c3e50;">
                Scene {i+1}<br>{scene_time:.1f}s
            </div>
            """
        
        html += """
                </div>
                <div style="margin-top: 60px;">
                    <h4 style="color: #2c3e50;">üìã Scene List</h4>
        """
        
        for i, scene_time in enumerate(scene_changes):
            next_scene = scene_changes[i + 1] if i + 1 < len(scene_changes) else None
            duration = (next_scene - scene_time) if next_scene else "remainder"
            
            time_str = time.strftime('%M:%S', time.gmtime(scene_time))
            
            html += f"""
            <div style="padding: 10px; border-bottom: 1px solid #eee; cursor: pointer;" 
                 onclick="
                    try {{
                        let video = document.querySelector('#vision_video_player video');
                        if (video) {{
                            video.currentTime = {scene_time};
                            console.log('Jumped to scene {i+1} at {scene_time}s');
                        }}
                    }} catch(e) {{ console.error(e); }}
                 ">
                <span style="background: #007bff; color: white; padding: 4px 8px; border-radius: 4px; margin-right: 10px;">
                    {time_str}
                </span>
                <span style="font-weight: 500;">Scene {i+1}</span>
                <span style="color: #666; margin-left: 10px;">
                    ({duration:.1f}s)" if isinstance(duration, (int, float)) else f"({duration})
                </span>
            </div>
            """
        
        html += """
                </div>
            </div>
            <p style="color: #666; font-size: 0.9em; margin-top: 10px;">üí° Click any scene to jump to that moment in the video!</p>
        </div>
        """
        
        return html
        
    except Exception as e:
        print(f"‚ùå Error generating scene analysis HTML: {str(e)}")
        return f"<p style='color: #e74c3c; text-align: center; padding: 40px;'>Error loading scene analysis: {str(e)}</p>"

def generate_silence_analysis_html():
    """Generate HTML visualization of silence analysis"""
    try:
        global current_vision_analysis
        
        if 'current_vision_analysis' not in globals() or not current_vision_analysis:
            return "<p style='color: #666; text-align: center; padding: 40px;'>No analysis data available. Please analyze a video first.</p>"
        
        silence_periods = current_vision_analysis['analysis']['silence_periods']
        video_title = current_vision_analysis['video_title']
        
        if not silence_periods:
            return "<p style='color: #666; text-align: center; padding: 40px;'>No silence periods detected in this video.</p>"
        
        total_silence = sum(period[2] for period in silence_periods)
        
        html = f"""
        <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <h3 style="color: #2c3e50; margin-bottom: 20px;">üîá Silence Analysis: {video_title}</h3>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-bottom: 20px;">
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <div style="font-size: 2em; margin-bottom: 5px;">üîá</div>
                    <div style="font-size: 1.5em; font-weight: bold; color: #e74c3c;">{len(silence_periods)}</div>
                    <div style="font-size: 0.9em; color: #666;">Silent Periods</div>
                </div>
                
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <div style="font-size: 2em; margin-bottom: 5px;">‚è±Ô∏è</div>
                    <div style="font-size: 1.5em; font-weight: bold; color: #e74c3c;">{total_silence:.1f}s</div>
                    <div style="font-size: 0.9em; color: #666;">Total Silence</div>
                </div>
                
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <div style="font-size: 2em; margin-bottom: 5px;">üìä</div>
                    <div style="font-size: 1.5em; font-weight: bold; color: #e74c3c;">{total_silence/60:.1f}min</div>
                    <div style="font-size: 0.9em; color: #666;">Time Savings</div>
                </div>
            </div>
            
            <div style="background: white; border-radius: 8px; padding: 20px; max-height: 400px; overflow-y: auto;">
                <h4 style="color: #2c3e50; margin-top: 0;">üìã Silent Periods (showing first 20)</h4>
        """
        
        # Show first 20 silence periods
        display_periods = silence_periods[:20]
        
        for i, (start, end, duration) in enumerate(display_periods):
            start_time = time.strftime('%M:%S', time.gmtime(start))
            end_time = time.strftime('%M:%S', time.gmtime(end))
            
            html += f"""
            <div style="padding: 8px; border-bottom: 1px solid #eee; font-family: monospace;">
                <span style="color: #666;">#{i+1:3d}</span>
                <span style="margin-left: 10px;">{start_time} - {end_time}</span>
                <span style="margin-left: 10px; color: #e74c3c;">({duration:.1f}s)</span>
            </div>
            """
        
        if len(silence_periods) > 20:
            html += f"<p style='color: #666; font-style: italic; margin-top: 10px;'>... and {len(silence_periods) - 20} more silent periods</p>"
        
        html += """
            </div>
            <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; margin-top: 15px;">
                <p style="margin: 0; color: #2e7d32;">
                    <strong>üí° Tip:</strong> Use the "Remove Silence" feature to automatically create a fast-paced version of your video!
                </p>
            </div>
        </div>
        """
        
        return html
        
    except Exception as e:
        print(f"‚ùå Error generating silence analysis HTML: {str(e)}")
        return f"<p style='color: #e74c3c; text-align: center; padding: 40px;'>Error loading silence analysis: {str(e)}</p>"

def generate_scene_analysis_html_direct(analysis_results, video_title):
    """Generate HTML visualization of scene analysis with direct data"""
    try:
        scene_changes = analysis_results.get('scene_changes', [])
        
        if not scene_changes:
            return "<p style='color: #666; text-align: center; padding: 40px;'>No scene changes detected in this video.</p>"
        
        html = f"""
        <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <h3 style="color: #2c3e50; margin-bottom: 20px;">üé≠ Scene Analysis: {video_title}</h3>
            <div style="background: white; border-radius: 8px; padding: 20px;">
                <h4 style="color: #2c3e50;">üìä Scene Timeline</h4>
                <div style="position: relative; height: 60px; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); border-radius: 8px; margin: 20px 0;">
        """
        
        # Calculate total duration for percentage positioning
        total_duration = scene_changes[-1] if scene_changes else 100
        
        for i, scene_time in enumerate(scene_changes):
            percentage = (scene_time / total_duration) * 100
            
            html += f"""
            <div style="position: absolute; left: {percentage}%; top: -5px; width: 2px; height: 70px; background: white; border-radius: 1px;"></div>
            <div style="position: absolute; left: {percentage}%; top: 75px; transform: translateX(-50%); font-size: 0.8em; color: #2c3e50;">
                Scene {i+1}<br>{scene_time:.1f}s
            </div>
            """
        
        html += """
                </div>
                <div style="margin-top: 60px;">
                    <h4 style="color: #2c3e50;">üìã Scene List</h4>
        """
        
        for i, scene_time in enumerate(scene_changes):
            next_scene = scene_changes[i + 1] if i + 1 < len(scene_changes) else None
            duration = (next_scene - scene_time) if next_scene else "remainder"
            
            time_str = time.strftime('%M:%S', time.gmtime(scene_time))
            
            html += f"""
            <div style="padding: 10px; border-bottom: 1px solid #eee; cursor: pointer;" 
                 onclick="
                    try {{
                        let video = document.querySelector('#vision_video_player video');
                        if (video) {{
                            video.currentTime = {scene_time};
                            console.log('Jumped to scene {i+1} at {scene_time}s');
                        }}
                    }} catch(e) {{ console.error(e); }}
                 ">
                <span style="background: #007bff; color: white; padding: 4px 8px; border-radius: 4px; margin-right: 10px;">
                    {time_str}
                </span>
                <span style="font-weight: 500;">Scene {i+1}</span>
                <span style="color: #666; margin-left: 10px;">
                    ({duration:.1f}s)" if isinstance(duration, (int, float)) else f"({duration})
                </span>
            </div>
            """
        
        html += """
                </div>
            </div>
            <p style="color: #666; font-size: 0.9em; margin-top: 10px;">üí° Click any scene to jump to that moment in the video!</p>
        </div>
        """
        
        return html
        
    except Exception as e:
        print(f"‚ùå Error generating scene analysis HTML: {str(e)}")
        return f"<p style='color: #e74c3c; text-align: center; padding: 40px;'>Error loading scene analysis: {str(e)}</p>"

def generate_silence_analysis_html_direct(analysis_results, video_title):
    """Generate HTML visualization of silence analysis with direct data"""
    try:
        silence_periods = analysis_results.get('silence_periods', [])
        
        if not silence_periods:
            return "<p style='color: #666; text-align: center; padding: 40px;'>No silence periods detected in this video.</p>"
        
        total_silence = sum(period[2] for period in silence_periods)
        
        html = f"""
        <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <h3 style="color: #2c3e50; margin-bottom: 20px;">üîá Silence Analysis: {video_title}</h3>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-bottom: 20px;">
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <div style="font-size: 2em; margin-bottom: 5px;">üîá</div>
                    <div style="font-size: 1.5em; font-weight: bold; color: #e74c3c;">{len(silence_periods)}</div>
                    <div style="font-size: 0.9em; color: #666;">Silent Periods</div>
                </div>
                
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <div style="font-size: 2em; margin-bottom: 5px;">‚è±Ô∏è</div>
                    <div style="font-size: 1.5em; font-weight: bold; color: #e74c3c;">{total_silence:.1f}s</div>
                    <div style="font-size: 0.9em; color: #666;">Total Silence</div>
                </div>
                
                <div style="background: white; padding: 15px; border-radius: 8px; text-align: center;">
                    <div style="font-size: 2em; margin-bottom: 5px;">üìä</div>
                    <div style="font-size: 1.5em; font-weight: bold; color: #e74c3c;">{total_silence/60:.1f}min</div>
                    <div style="font-size: 0.9em; color: #666;">Time Savings</div>
                </div>
            </div>
            
            <div style="background: white; border-radius: 8px; padding: 20px; max-height: 400px; overflow-y: auto;">
                <h4 style="color: #2c3e50; margin-top: 0;">üìã Silent Periods (showing first 20)</h4>
        """
        
        # Show first 20 silence periods
        display_periods = silence_periods[:20]
        
        for i, (start, end, duration) in enumerate(display_periods):
            start_time = time.strftime('%M:%S', time.gmtime(start))
            end_time = time.strftime('%M:%S', time.gmtime(end))
            
            html += f"""
            <div style="padding: 8px; border-bottom: 1px solid #eee; font-family: monospace;">
                <span style="color: #666;">#{i+1:3d}</span>
                <span style="margin-left: 10px;">{start_time} - {end_time}</span>
                <span style="margin-left: 10px; color: #e74c3c;">({duration:.1f}s)</span>
            </div>
            """
        
        if len(silence_periods) > 20:
            html += f"<p style='color: #666; font-style: italic; margin-top: 10px;'>... and {len(silence_periods) - 20} more silent periods</p>"
        
        html += """
            </div>
            <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; margin-top: 15px;">
                <p style="margin: 0; color: #2e7d32;">
                    <strong>üí° Tip:</strong> Use the "Remove Silence" feature to automatically create a fast-paced version of your video!
                </p>
            </div>
        </div>
        """
        
        return html
        
    except Exception as e:
        print(f"‚ùå Error generating silence analysis HTML: {str(e)}")
        return f"<p style='color: #e74c3c; text-align: center; padding: 40px;'>Error loading silence analysis: {str(e)}</p>"
    
# --- Object Detection Database ---
def init_detection_database():
    """Initialize SQLite database for storing object detections"""
    conn = sqlite3.connect('object_detections.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS detections (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            video_path TEXT,
            timestamp REAL,
            object_name TEXT,
            confidence REAL,
            bbox_x1 REAL,
            bbox_y1 REAL,
            bbox_x2 REAL,
            bbox_y2 REAL,
            frame_width INTEGER,
            frame_height INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_video_timestamp 
        ON detections(video_path, timestamp)
    ''')
    
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_object_name 
        ON detections(object_name)
    ''')
    
    conn.commit()
    conn.close()
    print("üìä Object detection database initialized")

# Initialize database on startup
init_detection_database()

# --- Real-Time Object Detection Class ---
class RealTimeObjectDetector:
    def __init__(self):
        self.model = None
        self.detection_active = False
        self.detection_thread = None
        self.detection_queue = queue.Queue()
        self.current_detections = []
        self.object_timeline = defaultdict(list)
        self.detection_history = deque(maxlen=1000)  # Keep last 1000 detections
        
        if YOLO_AVAILABLE:
            self.load_model()
    
    def load_model(self, model_size='yolo11n.pt'):
        """Load YOLOv11 model"""
        try:
            print(f"ü§ñ Loading YOLOv11 model: {model_size}")
            self.model = YOLO(model_size)
            
            # Warm up the model with a dummy image
            dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)
            self.model(dummy_frame)
            print("‚úÖ YOLOv11 model loaded and warmed up!")
            return True
        except Exception as e:
            print(f"‚ùå Error loading YOLO model: {str(e)}")
            return False
    
    def detect_objects_in_frame(self, frame, timestamp, video_path=None):
        """Detect objects in a single frame"""
        if not self.model:
            return []

        try:
            # Run detection using the correct YOLOv11 syntax
            results = self.model.predict(source=frame, verbose=False) # <-- Corrected prediction call

            frame_detections = []
            frame_height, frame_width = frame.shape[:2]

            # Get the first result object from the list
            result = results[0] # <-- New line to get the result object

            # Check if any boxes were detected in this result
            if result.boxes is not None: # <-- Check boxes on the 'result' object
                for box in result.boxes: # <-- Loop through boxes in the 'result' object
                    # Extract detection data with correct tensor-to-number conversion
                    confidence = float(box.conf.cpu().numpy()[0])
                    class_id = int(box.cls.cpu().numpy()[0])
                    object_name = self.model.names[class_id]

                    # Only keep high-confidence detections (your logic is correct here)
                    if confidence > 0.5:
                        bbox = box.xyxy.cpu().numpy()[0]
                        x1, y1, x2, y2 = bbox

                        detection = {
                            'timestamp': timestamp,
                            'object_name': object_name,
                            'confidence': confidence,
                            'bbox': [float(x1), float(y1), float(x2), float(y2)],
                            'frame_width': frame_width,
                            'frame_height': frame_height
                        }

                        frame_detections.append(detection)

                        # Store in database if video_path provided
                        if video_path:
                            self.store_detection(video_path, detection)

            # Update timeline (your logic is correct here)
            if frame_detections:
                for detection in frame_detections:
                    self.object_timeline[detection['object_name']].append({
                        'timestamp': timestamp,
                        'confidence': detection['confidence']
                    })

            # Update current detections for UI (your logic is correct here)
            self.current_detections = frame_detections
            self.detection_history.append({
                'timestamp': timestamp,
                'detections': frame_detections
            })

            return frame_detections

        except Exception as e:
            print(f"‚ùå Detection error: {str(e)}")
            return []
        
    def store_detection(self, video_path, detection):
        """Store detection in database"""
        try:
            conn = sqlite3.connect('object_detections.db')
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO detections 
                (video_path, timestamp, object_name, confidence, 
                 bbox_x1, bbox_y1, bbox_x2, bbox_y2, frame_width, frame_height)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                video_path,
                detection['timestamp'],
                detection['object_name'],
                detection['confidence'],
                detection['bbox'][0],
                detection['bbox'][1],
                detection['bbox'][2],
                detection['bbox'][3],
                detection['frame_width'],
                detection['frame_height']
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"‚ùå Database error: {str(e)}")
    
    def get_object_timeline_data(self, video_path, duration):
        """Get object timeline data for visualization"""
        try:
            conn = sqlite3.connect('object_detections.db')
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT object_name, timestamp, confidence
                FROM detections
                WHERE video_path = ?
                ORDER BY timestamp
            ''', (video_path,))
            
            results = cursor.fetchall()
            conn.close()
            
            # Group by object type
            timeline_data = defaultdict(list)
            for object_name, timestamp, confidence in results:
                timeline_data[object_name].append({
                    'timestamp': timestamp,
                    'confidence': confidence
                })
            
            return dict(timeline_data)
            
        except Exception as e:
            print(f"‚ùå Timeline data error: {str(e)}")
            return {}
    
    def search_objects(self, video_path, object_names, min_confidence=0.7):
        """Search for specific objects in video"""
        try:
            conn = sqlite3.connect('object_detections.db')
            cursor = conn.cursor()
            
            placeholders = ','.join(['?' for _ in object_names])
            query = f'''
                SELECT timestamp, object_name, confidence, 
                       bbox_x1, bbox_y1, bbox_x2, bbox_y2
                FROM detections
                WHERE video_path = ? AND object_name IN ({placeholders}) 
                AND confidence >= ?
                ORDER BY timestamp
            '''
            
            cursor.execute(query, [video_path] + object_names + [min_confidence])
            results = cursor.fetchall()
            conn.close()
            
            return [
                {
                    'timestamp': row[0],
                    'object_name': row[1],
                    'confidence': row[2],
                    'bbox': [row[3], row[4], row[5], row[6]]
                }
                for row in results
            ]
            
        except Exception as e:
            print(f"‚ùå Search error: {str(e)}")
            return []

# Global detector instance
global_detector = RealTimeObjectDetector()

# --- Enhanced Analysis Functions ---
def analyze_video_with_realtime_detection(video_path_str, history_data, detection_interval, confidence_threshold, progress=gr.Progress(track_tqdm=True)):
    """Enhanced video analysis with real-time object detection"""
    print("üî¨ Starting real-time object detection analysis")
    
    if not video_path_str or video_path_str == "No videos available":
        raise gr.Error("Please select a video from the dropdown first.")
    
    if not YOLO_AVAILABLE:
        raise gr.Error("YOLOv11 not available. Please install with: pip install ultralytics")
    
    selected_title = video_path_str.split("] ", 1)[1]
    video_item = next((item for item in history_data if item['title'] == selected_title), None)
    
    if not video_item:
        raise gr.Error("Could not find the selected video file.")
    
    video_path = video_item['local_filepath']
    
    if not os.path.exists(video_path):
        raise gr.Error(f"Video file not found at path: {video_path}")
    
    try:
        # Clear previous detections for this video
        conn = sqlite3.connect('object_detections.db')
        cursor = conn.cursor()
        cursor.execute('DELETE FROM detections WHERE video_path = ?', (video_path,))
        conn.commit()
        conn.close()
        
        progress(0.1, desc="üé¨ Opening video for analysis...")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise gr.Error("Could not open video file")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        progress(0.2, desc="ü§ñ Loading YOLOv11 model...")
        
        if not global_detector.model:
            success = global_detector.load_model()
            if not success:
                raise gr.Error("Failed to load YOLOv11 model")
        
        progress(0.3, desc="üîç Starting object detection...")
        
        # Detection parameters
        frame_interval = int(fps * detection_interval)  # Process every N seconds
        processed_frames = 0
        total_detections = 0
        unique_objects = set()
        
        # Process video frames
        for frame_num in range(0, total_frames, frame_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            ret, frame = cap.read()
            
            if not ret:
                break
            
            timestamp = frame_num / fps
            
            # Detect objects in frame
            detections = global_detector.detect_objects_in_frame(
                frame, timestamp, video_path
            )
            
            if detections:
                total_detections += len(detections)
                for det in detections:
                    if det['confidence'] >= confidence_threshold:
                        unique_objects.add(det['object_name'])
            
            processed_frames += 1
            
            # Update progress
            progress_val = 0.3 + (0.6 * frame_num / total_frames)
            progress(progress_val, 
                    desc=f"üîç Detected {len(unique_objects)} object types, {total_detections} total detections")
        
        cap.release()
        
        progress(0.9, desc="üìä Generating analysis results...")
        
        # Get timeline data
        timeline_data = global_detector.get_object_timeline_data(video_path, duration)
        
        # Store analysis globally
        global current_vision_analysis
        current_vision_analysis = {
            'video_path': video_path,
            'video_title': selected_title,
            'analysis': {
                'detection_data': timeline_data,
                'total_detections': total_detections,
                'unique_objects': list(unique_objects),
                'duration': duration,
                'fps': fps
            }
        }
        
        progress(1.0, desc="‚úÖ Real-time detection analysis complete!")
        
        # Generate results
        overview_html = generate_detection_overview(current_vision_analysis['analysis'], selected_title)
        timeline_html = generate_object_timeline_html(timeline_data, duration, selected_title)
        
        return (
            video_path,  # vision_video_player
            overview_html,  # analysis_overview
            timeline_html,  # object_timeline
            gr.update(interactive=True),  # search_objects_btn
            gr.update(interactive=True),  # create_object_clips_btn
            gr.update(interactive=True),  # export_detections_btn
            f"‚úÖ Detected {len(unique_objects)} object types with {total_detections} total detections!",  # status
            gr.update(choices=list(unique_objects), value=[], interactive=True),  # object_filter
        )
        
    except Exception as e:
        print(f"‚ùå Error in detection analysis: {str(e)}")
        raise gr.Error(f"Error during detection analysis: {str(e)}")

def generate_detection_overview(analysis_data, video_title):
    """Generate HTML overview of detection results"""
    detection_data = analysis_data.get('detection_data', {})
    total_detections = analysis_data.get('total_detections', 0)
    unique_objects = analysis_data.get('unique_objects', [])
    duration = analysis_data.get('duration', 0)
    
    # Calculate top objects
    object_counts = {}
    for obj_name, detections in detection_data.items():
        object_counts[obj_name] = len(detections)
    
    top_objects = sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    
    html = f"""
    <div style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;">
        <h2 style="margin: 0 0 20px 0; color: white;">ü§ñ Real-Time Detection Overview: {video_title}</h2>
        
        <div style="background: rgba(255,255,255,0.1); padding: 10px; border-radius: 8px; margin-bottom: 20px; text-align: center;">
            <strong>üöÄ YOLOv11 Powered Detection</strong> - State-of-the-art object recognition with real-time performance
        </div>
        
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-bottom: 20px;">
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üéØ</div>
                <div style="font-size: 1.5em; font-weight: bold;">{len(unique_objects)}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Object Types</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üìä</div>
                <div style="font-size: 1.5em; font-weight: bold;">{total_detections}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Total Detections</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">‚è±Ô∏è</div>
                <div style="font-size: 1.5em; font-weight: bold;">{duration/60:.1f}min</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Video Duration</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üîç</div>
                <div style="font-size: 1.5em; font-weight: bold;">{total_detections/duration:.1f}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Detections/sec</div>
            </div>
            
        </div>
        
        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;">
            <h3 style="margin: 0 0 10px 0; color: white;">üèÜ Top Detected Objects:</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px;">
    """
    
    for obj_name, count in top_objects:
        percentage = (count / total_detections * 100) if total_detections > 0 else 0
        html += f"""
        <div style="background: rgba(255,255,255,0.1); padding: 10px; border-radius: 8px; text-align: center;">
            <div style="font-weight: bold;">{obj_name.title()}</div>
            <div style="font-size: 1.2em; color: #ffd700;">{count}</div>
            <div style="font-size: 0.8em; opacity: 0.8;">({percentage:.1f}%)</div>
        </div>
        """
    
    html += """
            </div>
        </div>
        
        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; margin-top: 15px;">
            <h3 style="margin: 0 0 10px 0; color: white;">üéØ Available Actions:</h3>
            <ul style="margin: 0; padding-left: 20px; color: white;">
                <li>üîç Search for specific objects in timeline</li>
                <li>‚úÇÔ∏è Create clips containing specific objects</li>
                <li>üìä Export detection data for analysis</li>
                <li>üé¨ Jump to moments when objects appear</li>
            </ul>
        </div>
    </div>
    """
    
    return html

def generate_object_timeline_html(timeline_data, duration, video_title):
    """Generate interactive object timeline visualization"""
    if not timeline_data:
        return "<p style='color: #666; text-align: center; padding: 40px;'>No objects detected in this video.</p>"
    
    html = f"""
    <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
        <h3 style="color: #2c3e50; margin-bottom: 20px;">üé¨ Interactive Object Timeline: {video_title}</h3>
        
        <div style="background: white; border-radius: 8px; padding: 20px;">
            <h4 style="color: #2c3e50;">üìä Object Appearance Timeline</h4>
            <p style="color: #666; font-size: 0.9em;">Click on any timestamp to jump to that moment in the video!</p>
            
            <div style="margin: 20px 0;">
    """
    
    # Create timeline for each object type
    for obj_name, detections in timeline_data.items():
        # Calculate density for visualization
        timeline_segments = []
        for detection in detections:
            timestamp = detection['timestamp']
            confidence = detection['confidence']
            percentage = (timestamp / duration) * 100
            
            timeline_segments.append({
                'percentage': percentage,
                'timestamp': timestamp,
                'confidence': confidence
            })
        
        html += f"""
        <div style="margin-bottom: 20px; padding: 15px; background: #f8f9fa; border-radius: 8px;">
            <div style="display: flex; align-items: center; margin-bottom: 10px;">
                <span style="font-weight: bold; margin-right: 10px; min-width: 100px;">{obj_name.title()}</span>
                <span style="background: #007bff; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8em;">
                    {len(detections)} detections
                </span>
            </div>
            
            <div style="position: relative; height: 30px; background: #e9ecef; border-radius: 15px; overflow: hidden;">
        """
        
        # Add detection markers
        for segment in timeline_segments:
            opacity = min(1.0, segment['confidence'])
            html += f"""
            <div style="position: absolute; left: {segment['percentage']}%; top: 5px; width: 4px; height: 20px; 
                        background: rgba(0, 123, 255, {opacity}); border-radius: 2px; cursor: pointer;"
                 onclick="
                    try {{
                        let video = document.querySelector('#vision_video_player video');
                        if (video) {{
                            video.currentTime = {segment['timestamp']};
                            console.log('Jumped to {obj_name} at {segment['timestamp']:.1f}s');
                        }}
                    }} catch(e) {{ console.error(e); }}
                 "
                 title="{obj_name} at {segment['timestamp']:.1f}s (confidence: {segment['confidence']:.2f})">
            </div>
            """
        
        html += """
            </div>
        </div>
        """
    
    html += """
            </div>
        </div>
        
        <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; margin-top: 15px;">
            <p style="margin: 0; color: #2e7d32;">
                <strong>üí° Tips:</strong> 
                ‚Ä¢ Click blue markers to jump to object appearances
                ‚Ä¢ Darker markers = higher confidence detections
                ‚Ä¢ Use object filters below to focus on specific items
            </p>
        </div>
    </div>
    """
    
    return html

def search_objects_in_video(object_names, confidence_threshold):
    """Search for specific objects in the analyzed video"""
    global current_vision_analysis
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No analysis data available. Please analyze a video first.")
    
    if not object_names:
        return "Please select at least one object type to search for."
    
    video_path = current_vision_analysis['video_path']
    video_title = current_vision_analysis['video_title']
    
    # Search in database
    results = global_detector.search_objects(video_path, object_names, confidence_threshold)
    
    if not results:
        return f"No {', '.join(object_names)} found with confidence >= {confidence_threshold:.1f}"
    
    # Generate search results HTML
    html = f"""
    <div style="padding: 20px; background: #f8f9fa; border-radius: 10px; max-height: 400px; overflow-y: auto;">
        <h3 style="color: #2c3e50; margin-bottom: 15px;">üîç Search Results: {', '.join(object_names)}</h3>
        <p style="color: #666; margin-bottom: 20px;">Found {len(results)} detections in "{video_title}"</p>
    """
    
    for i, result in enumerate(results, 1):
        timestamp = result['timestamp']
        time_str = time.strftime('%M:%S', time.gmtime(timestamp))
        confidence = result['confidence']
        obj_name = result['object_name']
        
        html += f"""
        <div style="padding: 10px; margin-bottom: 10px; background: white; border-radius: 8px; 
                    border-left: 4px solid #007bff; cursor: pointer;"
             onclick="
                try {{
                    let video = document.querySelector('#vision_video_player video');
                    if (video) {{
                        video.currentTime = {timestamp};
                        console.log('Jumped to {obj_name} at {timestamp:.1f}s');
                    }}
                }} catch(e) {{ console.error(e); }}
             ">
            <div style="display: flex; justify-content: between; align-items: center;">
                <span style="font-weight: bold; color: #007bff;">[{time_str}]</span>
                <span style="margin-left: 10px; color: #2c3e50;">{obj_name.title()}</span>
                <span style="margin-left: auto; background: #28a745; color: white; padding: 2px 6px; 
                            border-radius: 10px; font-size: 0.8em;">
                    {confidence:.1%}
                </span>
            </div>
        </div>
        """
    
    html += """
    </div>
    <p style="color: #666; font-size: 0.9em; margin-top: 10px;">üí° Click any result to jump to that moment in the video!</p>
    """
    
    return html

def create_object_based_clips(object_names, clip_duration, min_confidence):
    """Create video clips containing specific objects"""
    global current_vision_analysis
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No analysis data available. Please analyze a video first.")
    
    if not object_names:
        raise gr.Error("Please select at least one object type.")
    
    video_path = current_vision_analysis['video_path']
    video_title = current_vision_analysis['video_title']
    
    # Find object detections
    detections = global_detector.search_objects(video_path, object_names, min_confidence)
    
    if not detections:
        raise gr.Error(f"No {', '.join(object_names)} found with sufficient confidence.")
    
    print(f"üé¨ Creating clips for {len(detections)} object detections...")
    
    # Group nearby detections to create meaningful clips
    clips_to_create = []
    current_clip_start = None
    current_clip_end = None
    
    for detection in detections:
        timestamp = detection['timestamp']
        
        if current_clip_start is None:
            # Start new clip
            current_clip_start = max(0, timestamp - clip_duration/2)
            current_clip_end = timestamp + clip_duration/2
        elif timestamp <= current_clip_end + 2:  # Extend if within 2 seconds
            current_clip_end = timestamp + clip_duration/2
        else:
            # Save current clip and start new one
            clips_to_create.append((current_clip_start, current_clip_end))
            current_clip_start = max(0, timestamp - clip_duration/2)
            current_clip_end = timestamp + clip_duration/2
    
    # Add final clip
    if current_clip_start is not None:
        clips_to_create.append((current_clip_start, current_clip_end))
    
    print(f"üìπ Creating {len(clips_to_create)} clips...")
    
    # Create clips
    created_files = []
    safe_title = safe_filename(video_title)
    object_names_str = "_".join(object_names)
    
    try:
        for i, (start_time, end_time) in enumerate(clips_to_create, 1):
            clip_filename = f"{safe_title}_{object_names_str}_clip_{i:02d}.mp4"
            clip_path = os.path.join(CLIPS_FOLDER, clip_filename)
            
            # Get GPU acceleration parameters
            decode_params = get_optimized_ffmpeg_params('decode')
            encode_params = get_optimized_ffmpeg_params('encode', 'h264')
            
            command = ["ffmpeg"]
            
            if decode_params:
                command.extend(decode_params)
            
            command.extend([
                "-i", video_path,
                "-ss", str(start_time),
                "-to", str(end_time)
            ])
            
            # For short clips, copy is usually faster
            duration = end_time - start_time
            if duration < 30:
                command.extend(["-c", "copy"])
            else:
                command.extend(encode_params)
                command.extend(["-c:a", "aac"])
            
            command.extend(["-y", clip_path])
            
            subprocess.run(command, check=True, capture_output=True, text=True)
            
            if os.path.exists(clip_path):
                created_files.append((clip_path, f"Clip {i}: {duration:.1f}s"))
                print(f"‚úÖ Created clip {i}/{len(clips_to_create)}")
        
        # Create zip file
        import zipfile
        zip_filename = f"{safe_title}_{object_names_str}_clips.zip"
        zip_path = os.path.join(CLIPS_FOLDER, zip_filename)
        
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            for file_path, description in created_files:
                zipf.write(file_path, os.path.basename(file_path))
        
        return (
            gr.File(value=zip_path, visible=True),
            f"‚úÖ Created {len(created_files)} clips containing {', '.join(object_names)}! Total duration: {sum(end-start for start, end in clips_to_create):.1f}s"
        )
        
    except Exception as e:
        print(f"‚ùå Error creating clips: {str(e)}")
        raise gr.Error(f"Error creating clips: {str(e)}")

def export_detection_data():
    """Export object detection data as CSV"""
    global current_vision_analysis
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No analysis data available. Please analyze a video first.")
    
    video_path = current_vision_analysis['video_path']
    video_title = current_vision_analysis['video_title']
    
    try:
        conn = sqlite3.connect('object_detections.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT timestamp, object_name, confidence, 
                   bbox_x1, bbox_y1, bbox_x2, bbox_y2
            FROM detections
            WHERE video_path = ?
            ORDER BY timestamp, object_name
        ''', (video_path,))
        
        results = cursor.fetchall()
        conn.close()
        
        if not results:
            raise gr.Error("No detection data found for this video.")
        
        # Create CSV content
        csv_content = "timestamp,object_name,confidence,bbox_x1,bbox_y1,bbox_x2,bbox_y2\n"
        for row in results:
            csv_content += f"{row[0]:.2f},{row[1]},{row[2]:.3f},{row[3]:.1f},{row[4]:.1f},{row[5]:.1f},{row[6]:.1f}\n"
        
        # Save to file
        safe_title = safe_filename(video_title)
        csv_filename = f"{safe_title}_detections.csv"
        csv_path = os.path.join(OUTPUT_FOLDER, csv_filename)
        
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write(csv_content)
        
        return (
            gr.File(value=csv_path, visible=True),
            f"‚úÖ Exported {len(results)} detections to CSV file!"
        )
        
    except Exception as e:
        print(f"‚ùå Export error: {str(e)}")
        raise gr.Error(f"Error exporting data: {str(e)}")           
    
class EnhancedRealTimeDetector:
    def __init__(self):
        self.model = None
        self.cap = None
        self.current_video_path = None
        self.detection_active = False
        self.current_detections = []
            
        if YOLO_AVAILABLE:
            self.load_model()
    
    def load_model(self, model_size='yolo11s.pt'):
        """Load YOLOv11 model with proper testing"""
        try:
            print(f"ü§ñ Loading YOLOv11 model: {model_size}")
            self.model = YOLO(model_size)
            
            # IMPORTANT: Test the model immediately
            print("üß™ Testing model...")
            dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)
            test_results = self.model(dummy_frame, verbose=False)
            
            if test_results:
                print("‚úÖ YOLOv11 model loaded and tested successfully!")
                return True
            else:
                print("‚ùå Model test failed")
                return False
                
        except Exception as e:
            print(f"‚ùå Error loading YOLO model: {str(e)}")
            self.model = None
            return False

    def open_video(self, video_path):
        """Open video file and prepare for detection"""
        try:
            if self.cap:
                self.cap.release()
            
            print(f"üé¨ Opening video: {video_path}")
            self.cap = cv2.VideoCapture(video_path)
            
            if not self.cap.isOpened():
                print(f"‚ùå Could not open video file: {video_path}")
                return False
            
            # Get video info
            fps = self.cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            print(f"‚úÖ Video opened: {fps:.1f} FPS, {frame_count} frames, {duration:.1f}s")
            
            self.current_video_path = video_path
            self.detection_active = True
            return True
            
        except Exception as e:
            print(f"‚ùå Error opening video: {str(e)}")
            return False

    def get_frame_at_timestamp(self, timestamp_str, confidence_threshold):
        """OPTIMIZED: Faster frame processing with smart filtering"""
        if not self.cap or not self.model or not timestamp_str:
            return None

        try:
            timestamp = float(timestamp_str)
            
            # SPEED OPTIMIZATION: Only seek if timestamp changed significantly
            if not hasattr(self, '_last_timestamp') or abs(timestamp - self._last_timestamp) > 0.2:
                self.cap.set(cv2.CAP_PROP_POS_MSEC, timestamp * 1000)
                self._last_timestamp = timestamp
            
            ret, frame = self.cap.read()
            if not ret:
                return None

            # SPEED OPTIMIZATION: Resize frame for detection (keep display quality)
            detection_frame = cv2.resize(frame, (640, 480))  # Smaller = faster detection
            
            # Run detection on smaller frame
            results = self.model.predict(
                source=detection_frame, 
                conf=confidence_threshold,
                verbose=False,
                save=False,
                show=False,
                device='0' if torch.cuda.is_available() else 'cpu'  # Force GPU if available
            )
            
            # Work on original frame for display
            annotated_frame = frame.copy()
            detections = []
            
            if results and len(results) > 0:
                result = results[0]
                if result.boxes is not None and len(result.boxes) > 0:
                    
                    for i, box in enumerate(result.boxes):
                        try:
                            confidence = float(box.conf.cpu().numpy()[0])
                            class_id = int(box.cls.cpu().numpy()[0])
                            object_name = self.model.names[class_id]
                            
                            # ACCURACY FIX: Smart filtering for common misclassifications
                            if not self.is_detection_valid(object_name, confidence, confidence_threshold):
                                continue
                            
                            # Scale bounding box back to original frame size
                            bbox = box.xyxy[0].cpu().numpy()
                            scale_x = frame.shape[1] / 640
                            scale_y = frame.shape[0] / 480
                            
                            x1 = int(bbox[0] * scale_x)
                            y1 = int(bbox[1] * scale_y)
                            x2 = int(bbox[2] * scale_x)
                            y2 = int(bbox[3] * scale_y)
                            
                            detections.append({
                                'object_name': object_name, 
                                'confidence': confidence, 
                                'bbox': [x1, y1, x2, y2]
                            })
                            
                            # Draw bounding box with confidence-based styling
                            color = self.get_confidence_color(confidence)
                            thickness = 3 if confidence > 0.7 else 2
                            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, thickness)
                            
                            # With this enhanced version:
                            display_name = self.get_display_name(object_name)
                            label = f"{display_name}: {confidence:.0%}"
                            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                            
                            # Background with transparency effect
                            cv2.rectangle(annotated_frame, 
                                        (x1, y1 - label_size[1] - 10), 
                                        (x1 + label_size[0] + 10, y1), 
                                        color, -1)
                            
                            # Text with better contrast
                            cv2.putText(annotated_frame, label, (x1 + 5, y1 - 5), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                            
                        except Exception as e:
                            continue
            
            self.current_detections = detections
            
            # Convert to RGB for Gradio display
            return cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        
        except Exception as e:
            print(f"‚ùå Error processing frame: {e}")
            return None

def __init__(self):
    self.model = None
    self.cap = None
    self.current_video_path = None
    self.detection_active = False
    self.current_detections = []
    
    # AUTO-DETECT ALL OBJECTS: Complete YOLO11 class list
    self.all_yolo_classes = [
        "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
        "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", 
        "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", 
        "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", 
        "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", 
        "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", 
        "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", 
        "chair", "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", 
        "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", 
        "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
    ]
    
    # ENHANCED OBJECT MAPPING: Map similar objects to more descriptive names
    self.object_aliases = {
        "tv": "Monitor/TV",           # TVs often detect computer monitors
        "laptop": "Laptop/PC",        # Laptops are computers
        "cell phone": "Phone/Mobile", # More descriptive
        "dining table": "Table/Desk", # Tables often used as desks
        "chair": "Chair/Office Chair", # More descriptive
        "mouse": "Computer Mouse",    # Distinguish from animal
        "keyboard": "Keyboard",       # Computer keyboard
        "remote": "Remote Control",   # More descriptive
        "book": "Book/Document",      # Could be documents
        "bottle": "Bottle/Container", # More general
        "cup": "Cup/Mug",            # More descriptive
        "clock": "Clock/Timer",       # More general
        "potted plant": "Plant",      # Simpler name
        "sports ball": "Ball",        # Simpler name
        "wine glass": "Glass",        # More general
        "cell phone": "Phone",        # Shorter name
        "teddy bear": "Toy/Teddy Bear", # More general
    }
    
    # SMART CONFIDENCE: Higher thresholds for commonly confused objects
    self.confidence_thresholds = {
        "person": 0.65,              # People need high confidence
        "car": 0.55,                 # Cars vs trucks confusion
        "truck": 0.65,               # Trucks vs cars confusion  
        "bus": 0.70,                 # Buses vs trucks confusion
        "motorcycle": 0.70,          # vs bicycle confusion
        "bicycle": 0.60,             # vs motorcycle confusion
        "airplane": 0.80,            # vs bird confusion
        "bird": 0.75,                # vs airplane confusion
        "cat": 0.65,                 # vs dog confusion
        "dog": 0.65,                 # vs cat confusion
        "tv": 0.50,                  # Good for monitor detection
        "laptop": 0.60,              # Good detection usually
        "cell phone": 0.70,          # Small object, needs confidence
        "mouse": 0.75,               # Very small object
        "keyboard": 0.65,            # Rectangular, can confuse
        "chair": 0.55,               # Good detection usually
        "dining table": 0.50,        # Tables/desks detect well
        "book": 0.70,                # Small objects need confidence
        "clock": 0.75,               # Can be confused with other round objects
        "bottle": 0.60,              # Cylindrical objects
        "cup": 0.70,                 # Small objects
        "remote": 0.75,              # Very small object
    }
    
    self.use_smart_confidence = True
    
    if YOLO_AVAILABLE:
        self.load_model()

def is_detection_valid(self, object_name, confidence, base_threshold):
    """ENHANCED: Smart filtering with automatic confidence adjustment"""
    
    if not self.use_smart_confidence:
        return confidence >= base_threshold
    
    # Get smart threshold for this object (or use base if not specified)
    required_confidence = self.confidence_thresholds.get(object_name, base_threshold)
    
    if confidence < required_confidence:
        return False
    
    # CONTEXT-BASED FILTERING: Remove obviously wrong detections
    
    # Size-based filtering (very rough estimates)
    bbox_area = 0  # You could calculate this if needed
    
    # Filter out impossible scenarios (expand as needed)
    outdoor_only = ['airplane', 'train', 'bus', 'truck', 'traffic light', 'fire hydrant', 'stop sign']
    large_objects = ['airplane', 'train', 'bus', 'truck', 'car', 'boat']
    
    # For now, keep it simple and just use confidence
    return True

def get_display_name(self, object_name):
    """Get user-friendly display name for detected objects"""
    return self.object_aliases.get(object_name, object_name.title())

def get_confidence_color(self, confidence):
    """Get color based on confidence level"""
    if confidence > 0.8:
        return (0, 255, 0)      # Green for high confidence
    elif confidence > 0.6:
        return (0, 165, 255)    # Orange for medium confidence  
    else:
        return (0, 0, 255)      # Red for low confidence
        
    def get_class_color(self, class_id):
        """Get consistent color for object class"""
        # Use class_id to generate consistent colors
        np.random.seed(class_id + 42)  # +42 for better color distribution
        return tuple(map(int, np.random.randint(50, 255, 3)))   
            
def detect_frame_with_overlay(self, frame, confidence_threshold):
        """FIXED: Proper tensor handling and debugging"""
        if not self.model:
            return frame, []
        
        try:
            # Run YOLO with proper parameters
            results = self.model.predict(
                source=frame, 
                conf=confidence_threshold,
                verbose=False,
                save=False,
                show=False
            )
            
            overlay_frame = frame.copy()
            detections = []
            
            if not results or len(results) == 0:
                return overlay_frame, detections
            
            result = results[0]
            if result.boxes is None or len(result.boxes) == 0:
                return overlay_frame, detections

            print(f"üì¶ Processing {len(result.boxes)} boxes")

            for i, box in enumerate(result.boxes):
                try:
                    # CRITICAL FIX: Proper tensor handling
                    confidence = float(box.conf.cpu().numpy()[0])
                    class_id = int(box.cls.cpu().numpy()[0])
                    object_name = self.model.names[class_id]
                    
                    if confidence >= confidence_threshold:
                        # CRITICAL FIX: Proper bbox extraction
                        bbox = box.xyxy[0].cpu().numpy()
                        x1, y1, x2, y2 = map(int, bbox)
                        
                        detections.append({
                            'object_name': object_name, 
                            'confidence': confidence, 
                            'bbox': [x1, y1, x2, y2]
                        })
                        
                        # Draw bounding box
                        color = self.get_class_color(class_id)
                        cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), color, 3)
                        
                        # Draw label
                        label = f"{object_name}: {confidence:.0%}"
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                        
                        # Background for text
                        cv2.rectangle(overlay_frame, 
                                    (x1, y1 - label_size[1] - 10), 
                                    (x1 + label_size[0], y1), 
                                    color, -1)
                        
                        # Text
                        cv2.putText(overlay_frame, label, (x1, y1 - 5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        
                        print(f"  ‚úÖ {object_name}: {confidence:.0%}")
                        
                except Exception as e:
                    print(f"‚ùå Error processing box {i}: {e}")
                    continue
            
            return overlay_frame, detections
            
        except Exception as e:
            print(f"‚ùå Detection error: {e}")
            return frame, []

def get_class_color(self, class_id):
        """Get color for object class"""
        np.random.seed(class_id + 42)
        return tuple(map(int, np.random.randint(80, 255, 3)))

def test_yolo11_detection():
    """Test YOLO11 detection with a simple image to verify it works"""
    global enhanced_detector
    
    if not enhanced_detector or not enhanced_detector.model:
        print("‚ùå No model loaded for testing")
        return False
    
    try:
        print("üß™ Testing YOLO11 detection with sample image...")
        
        # Create a test image with some basic shapes
        test_image = np.zeros((480, 640, 3), dtype=np.uint8)
        # Add some colored rectangles that might trigger detections
        cv2.rectangle(test_image, (100, 100), (200, 200), (255, 255, 255), -1)  # White rectangle
        cv2.rectangle(test_image, (300, 150), (400, 250), (128, 128, 128), -1)  # Gray rectangle
        
        # Test with very low confidence to see if anything gets detected
        results = enhanced_detector.model.predict(
            source=test_image,
            conf=0.1,  # Very low confidence for testing
            verbose=True,  # Enable verbose for debugging
            save=False
        )
        
        if results and len(results) > 0:
            result = results[0]
            if result.boxes is not None and len(result.boxes) > 0:
                print(f"‚úÖ Model working! Found {len(result.boxes)} detections in test image")
                for box in result.boxes:
                    conf = float(box.conf.cpu().numpy()[0])
                    cls = int(box.cls.cpu().numpy()[0])
                    name = enhanced_detector.model.names[cls]
                    print(f"  üéØ Test detection: {name} ({conf:.2f})")
                return True
            else:
                print("‚ö†Ô∏è Model working but no detections in test image (normal)")
                return True
        else:
            print("‚ùå No results from model - something wrong")
            return False
            
    except Exception as e:
        print(f"‚ùå Model test failed: {str(e)}")
        return False

def generate_enhanced_detection_overview(analysis_data, video_title):
    """Generate enhanced HTML overview with model info"""
    detection_data = analysis_data.get('detection_data', {})
    total_detections = analysis_data.get('total_detections', 0)
    unique_objects = analysis_data.get('unique_objects', [])
    duration = analysis_data.get('duration', 0)
    confidence_threshold = analysis_data.get('confidence_threshold', 0.6)
    model_size = analysis_data.get('model_size', 'yolo11s.pt')
    
    # Calculate detection density
    detection_density = total_detections / duration if duration > 0 else 0
    
    # Calculate top objects
    object_counts = {}
    for obj_name, detections in detection_data.items():
        object_counts[obj_name] = len(detections)
    
    top_objects = sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    
    # Model info
    model_info = {
    'yolo11n.pt': 'Nano - Fastest, Lower Accuracy',
    'yolo11s.pt': 'Small - Balanced Speed & Accuracy',
    'yolo11m.pt': 'Medium - Higher Accuracy',
    'yolo11l.pt': 'Large - Very High Accuracy',
    'yolo11x.pt': 'Extra Large - Maximum Accuracy'
}
    
    html = f"""
    <div style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;">
        <h2 style="margin: 0 0 20px 0; color: white;">ü§ñ Enhanced Real-Time Detection: {video_title}</h2>
        
        <div style="background: rgba(255,255,255,0.1); padding: 10px; border-radius: 8px; margin-bottom: 20px; text-align: center;">
            <strong>üöÄ {model_info.get(model_size, model_size)}</strong> ‚Ä¢ 
            <strong>üéØ {confidence_threshold:.0%} Confidence</strong> ‚Ä¢ 
            <strong>üì∫ Real-Time Overlay Active</strong>
        </div>
        
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 15px; margin-bottom: 20px;">
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üéØ</div>
                <div style="font-size: 1.5em; font-weight: bold;">{len(unique_objects)}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Object Types</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üìä</div>
                <div style="font-size: 1.5em; font-weight: bold;">{total_detections}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Total Detections</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">‚ö°</div>
                <div style="font-size: 1.5em; font-weight: bold;">{detection_density:.1f}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Detections/sec</div>
            </div>
            
            <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; text-align: center;">
                <div style="font-size: 2em; margin-bottom: 5px;">üé¨</div>
                <div style="font-size: 1.5em; font-weight: bold;">{confidence_threshold:.0%}</div>
                <div style="font-size: 0.9em; opacity: 0.9;">Accuracy Filter</div>
            </div>
            
        </div>
        
        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px;">
            <h3 style="margin: 0 0 10px 0; color: white;">üèÜ Top Detected Objects:</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(120px, 1fr)); gap: 10px;">
    """
    
    for obj_name, count in top_objects:
        percentage = (count / total_detections * 100) if total_detections > 0 else 0
        html += f"""
        <div style="background: rgba(255,255,255,0.1); padding: 10px; border-radius: 8px; text-align: center;">
            <div style="font-weight: bold; font-size: 0.9em;">{obj_name.title()}</div>
            <div style="font-size: 1.2em; color: #ffd700;">{count}</div>
            <div style="font-size: 0.8em; opacity: 0.8;">({percentage:.1f}%)</div>
        </div>
        """
    
    html += """
            </div>
        </div>
        
        <div style="background: rgba(46, 204, 113, 0.2); padding: 15px; border-radius: 10px; margin-top: 15px;">
            <h3 style="margin: 0 0 10px 0; color: white;">üéØ Enhanced Features Active:</h3>
            <ul style="margin: 0; padding-left: 20px; color: white; font-size: 0.9em;">
                <li>üîç Real-time bounding box overlay on video</li>
                <li>üìä Enhanced accuracy with confidence filtering</li>
                <li>üé¨ Live detection feed with current objects</li>
                <li>‚úÇÔ∏è Smart object-based clipping</li>
                <li>üìà Interactive timeline with jump-to-moment</li>
            </ul>
        </div>
    </div>
    """
    
    return html

def create_live_detection_overlay():
    """Create HTML for live detection overlay"""
    html = f"""
    <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
        <h3 style="color: #2c3e50; margin-bottom: 20px;">üì∫ Live Detection Overlay</h3>
        
        <div style="background: white; border-radius: 8px; padding: 20px;">
            <div style="text-align: center; margin-bottom: 20px;">
                <div id="overlay-frame-container" style="position: relative; display: inline-block; max-width: 100%;">
                    <div id="overlay-placeholder" style="width: 640px; height: 360px; background: linear-gradient(45deg, #f0f0f0, #e0e0e0); 
                                                        border-radius: 8px; display: flex; align-items: center; justify-content: center; border: 2px dashed #ccc;">
                        <div style="text-align: center; color: #666;">
                            <div style="font-size: 3em; margin-bottom: 10px;">üé¨</div>
                            <div style="font-size: 1.2em; font-weight: bold;">Real-Time Detection Overlay</div>
                            <div style="font-size: 0.9em; margin-top: 5px;">Play video to see live object detection</div>
                        </div>
                    </div>
                    <canvas id="detection-overlay" width="640" height="360" 
                            style="position: absolute; top: 0; left: 0; pointer-events: none; display: none;"></canvas>
                </div>
            </div>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
                    <h4 style="color: #2c3e50; margin: 0 0 10px 0;">üéØ Current Detections</h4>
                    <div id="current-detections" style="max-height: 200px; overflow-y: auto;">
                        <div style="color: #666; font-style: italic; text-align: center; padding: 20px;">
                            No detections yet...
                        </div>
                    </div>
                </div>
                
                <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
                    <h4 style="color: #2c3e50; margin: 0 0 10px 0;">üìä Detection Stats</h4>
                    <div id="detection-stats">
                        <div style="margin: 5px 0;">Objects Found: <span id="objects-found">0</span></div>
                        <div style="margin: 5px 0;">Avg Confidence: <span id="avg-confidence">0%</span></div>
                        <div style="margin: 5px 0;">Frame Rate: <span id="detection-fps">0</span> FPS</div>
                    </div>
                </div>
            </div>
        </div>
        
        <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; margin-top: 15px;">
            <p style="margin: 0; color: #1976d2;">
                <strong>üí° Live Detection:</strong> 
                Bounding boxes and labels will appear on the video as objects are detected in real-time!
            </p>
        </div>
    </div>
    
    <script>
    let detectionUpdateInterval;
    let lastDetectionTime = 0;
    let detectionCount = 0;
    
    function startLiveDetectionUpdates() {{
        console.log("üöÄ Starting live detection updates");
        
        if (detectionUpdateInterval) {{
            clearInterval(detectionUpdateInterval);
        }}
        
        detectionUpdateInterval = setInterval(updateDetectionOverlay, 200); // 5 FPS
    }}
    
    function updateDetectionOverlay() {{
        // This would be called from Python backend with current detection data
        // For now, we'll simulate it for the UI
        const detections = [
            {{object_name: 'Person', confidence: 0.95}},
            {{object_name: 'Laptop', confidence: 0.87}},
            {{object_name: 'Phone', confidence: 0.82}}
        ];
        
        // Update detection display (this will be connected to real data later)
        updateDetectionDisplay(detections);
    }}
    
    function updateDetectionDisplay(detections) {{
        const container = document.getElementById('current-detections');
        
        if (detections && detections.length > 0) {{
            let html = '';
            let totalConfidence = 0;
            
            detections.forEach(detection => {{
                const confidence = (detection.confidence * 100).toFixed(1);
                totalConfidence += detection.confidence;
                
                html += `
                <div style="padding: 8px; margin: 5px 0; background: white; border-radius: 5px; border-left: 3px solid #007bff;">
                    <div style="font-weight: bold;">${{detection.object_name}}</div>
                    <div style="font-size: 0.9em; color: #666;">Confidence: ${{confidence}}%</div>
                </div>
                `;
            }});
            
            container.innerHTML = html;
            
            // Update stats
            const avgConfidence = ((totalConfidence / detections.length) * 100).toFixed(1);
            document.getElementById('objects-found').textContent = detections.length;
            document.getElementById('avg-confidence').textContent = avgConfidence + '%';
            
            // Update FPS
            const now = Date.now();
            if (lastDetectionTime > 0) {{
                const fps = (1000 / (now - lastDetectionTime)).toFixed(1);
                document.getElementById('detection-fps').textContent = fps;
            }}
            lastDetectionTime = now;
            
        }} else {{
            container.innerHTML = '<div style="color: #666; font-style: italic; text-align: center; padding: 20px;">No objects detected in current frame</div>';
        }}
    }}
    
    // Start updates when page loads
    setTimeout(startLiveDetectionUpdates, 1000);
    
    console.log("‚úÖ Live detection overlay script loaded");
    </script>
    """
    
    return html

def get_live_detection_status():
    """Get current detection status and results"""
    global enhanced_detector
    
    if not enhanced_detector or not enhanced_detector.detection_active:
        return {
            'status': 'inactive',
            'detections': [],
            'stats': {'objects_found': 0, 'avg_confidence': 0, 'fps': 0}
        }
    
    detections = enhanced_detector.current_detections or []
    
    # Calculate stats
    avg_confidence = 0
    if detections:
        avg_confidence = sum(d['confidence'] for d in detections) / len(detections)
    
    return {
        'status': 'active',
        'detections': detections,
        'stats': {
            'objects_found': len(detections),
            'avg_confidence': avg_confidence * 100,
            'fps': enhanced_detector.video_fps if enhanced_detector.video_fps else 0
        }
    }

def update_detection_display():
    """Update the detection display with current results"""
    global enhanced_detector
    
    status = get_live_detection_status()
    
    if status['status'] == 'inactive':
        return "üî¥ Detection inactive - analyze a video first"
    
    detections = status['detections']
    stats = status['stats']
    
    if not detections:
        return f"""
        <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
            <h4 style="color: #2c3e50;">üéØ Live Detection Status</h4>
            <div style="background: white; padding: 15px; border-radius: 8px;">
                <div style="color: #666; text-align: center;">
                    üü¢ Detection Active - No objects in current frame
                </div>
                <div style="margin-top: 10px; font-size: 0.9em; color: #666;">
                    Video FPS: {stats['fps']:.1f} | Confidence Threshold: 70%
                </div>
            </div>
        </div>
        """
    
    # Build detection list
    detection_html = ""
    for detection in detections:
        confidence = detection['confidence'] * 100
        detection_html += f"""
        <div style="padding: 8px; margin: 5px 0; background: white; border-radius: 5px; border-left: 3px solid #007bff;">
            <div style="font-weight: bold;">{detection['object_name'].title()}</div>
            <div style="font-size: 0.9em; color: #666;">Confidence: {confidence:.1f}%</div>
        </div>
        """
    
    return f"""
    <div style="padding: 20px; background: #f8f9fa; border-radius: 10px;">
        <h4 style="color: #2c3e50;">üéØ Live Detection Feed</h4>
        <div style="background: white; padding: 15px; border-radius: 8px;">
            <div style="margin-bottom: 15px;">
                <span style="background: #28a745; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                    üü¢ LIVE: {len(detections)} objects detected
                </span>
            </div>
            
            <div style="max-height: 200px; overflow-y: auto;">
                {detection_html}
            </div>
            
            <div style="margin-top: 15px; padding-top: 10px; border-top: 1px solid #eee; font-size: 0.9em; color: #666;">
                üìä Stats: {stats['objects_found']} objects | {stats['avg_confidence']:.1f}% avg confidence | {stats['fps']:.1f} FPS
            </div>
        </div>
    </div>
    """

# Helper functions for live detection feed
def start_live_detection_feed():
    """Start the live detection feed with better status updates"""
    global enhanced_detector
    
    if enhanced_detector and enhanced_detector.video_path:
        if not enhanced_detector.detection_active:
            # Start detection if not already running
            enhanced_detector.start_realtime_detection(enhanced_detector.video_path, 0.7)
            print("üöÄ Live detection feed started")
        
        # Update the overlay with current status
        status_html = update_detection_display()
        
        return (
            gr.update(value="üì∫ Live Detection Active", variant="primary"),
            status_html
        )
    else:
        print("‚ùå No video analyzed yet")
        return (
            gr.update(value="‚ùå Analyze video first", variant="secondary"),
            "üî¥ No video analysis available. Please analyze a video first."
        )

def stop_live_detection_feed():
    """Stop the live detection feed"""
    global enhanced_detector
    
    if enhanced_detector:
        enhanced_detector.stop_detection()
        print("‚èπÔ∏è Live detection stopped")
        return (
            gr.update(value="üì∫ Start Live Detection Feed", variant="secondary"),
            "üî¥ Live detection stopped"
        )
    
    return (
        gr.update(value="üì∫ Start Live Detection Feed", variant="secondary"),
        "üî¥ No detection to stop"
    ) 

def export_analysis_data():
    """Export comprehensive analysis data"""
    global current_vision_analysis
    
    print("üìÅ Exporting analysis data...")
    
    if 'current_vision_analysis' not in globals() or not current_vision_analysis:
        raise gr.Error("No analysis data available. Please analyze a video first.")
    
    video_title = current_vision_analysis['video_title']
    analysis = current_vision_analysis['analysis']
    
    # Create comprehensive analysis report
    safe_title = re.sub(r'[<>:"/\\|?*]', '_', video_title)
    
    # JSON Export
    json_filename = f"{safe_title}_analysis.json"
    json_path = os.path.join(OUTPUT_FOLDER, json_filename)
    
    export_data = {
        'video_title': video_title,
        'analysis_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'summary': {
            'frames_analyzed': len(analysis.get('frames', [])),
            'scene_changes': len(analysis.get('scene_changes', [])),
            'silence_periods': len(analysis.get('silence_periods', [])),
            'total_silence_duration': sum(p[2] for p in analysis.get('silence_periods', []))
        },
        'scene_changes': analysis.get('scene_changes', []),
        'silence_periods': analysis.get('silence_periods', []),
        'frame_quality_scores': [
            {
                'timestamp': item['timestamp'],
                'score': item['score'],
                'sharpness': item['sharpness'],
                'contrast': item['contrast'],
                'brightness': item['brightness']
            } for item in analysis.get('frame_quality', [])
        ]
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, indent=2)
    
    print(f"‚úÖ Analysis data exported to {json_filename}")
    
    return (
        gr.File(value=json_path, visible=True),
        f"‚úÖ Analysis data exported to: downloads/{json_filename}"
    )

def format_timestamp(seconds):
    """Convert seconds to SRT timestamp format (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

def format_vtt_timestamp(seconds):
    """Convert seconds to WebVTT timestamp format (HH:MM:SS.mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}.{millisecs:03d}"

def generate_srt_content(segments):
    """Generate SRT subtitle content from Whisper segments"""
    srt_content = ""
    for i, segment in enumerate(segments, 1):
        start_time = format_timestamp(segment['start'])
        end_time = format_timestamp(segment['end'])
        text = segment['text'].strip()
        srt_content += f"{i}\n{start_time} --> {end_time}\n{text}\n\n"
    return srt_content

def generate_vtt_content(segments):
    """Generate WebVTT subtitle content from Whisper segments"""
    vtt_content = "WEBVTT\n\n"
    for segment in segments:
        start_time = format_vtt_timestamp(segment['start'])
        end_time = format_vtt_timestamp(segment['end'])
        text = segment['text'].strip()
        vtt_content += f"{start_time} --> {end_time}\n{text}\n\n"
    return vtt_content

def search_transcript(segments, search_query):
    """Search for a query in transcript segments and return matching results"""
    if not search_query or not search_query.strip():
        return []
    
    search_query = search_query.lower().strip()
    results = []
    
    for i, segment in enumerate(segments):
        text = segment['text'].lower()
        if search_query in text:
            # Highlight the search term in the original text
            highlighted_text = re.sub(
                re.escape(search_query), 
                f"<mark>{search_query}</mark>", 
                segment['text'], 
                flags=re.IGNORECASE
            )
            results.append({
                'segment_index': i,
                'start_time': segment['start'],
                'end_time': segment['end'],
                'text': segment['text'],
                'highlighted_text': highlighted_text
            })
    
    return results

def transcribe_video(video_path_str, history_data, whisper_model_size, progress=gr.Progress(track_tqdm=True)):
    print("üé¨ Starting transcribe_video function")
    print(f"üìπ Video path string: {video_path_str}")
    
    if not video_path_str or video_path_str == "No videos available":
        print("‚ùå No video selected")
        raise gr.Error("Please select a video from the dropdown first.")
    
    selected_title = video_path_str.split("] ", 1)[1]
    print(f"üè∑Ô∏è Selected title: {selected_title}")
    
    video_item = next((item for item in history_data if item['title'] == selected_title), None)
    if not video_item:
        print("‚ùå Video item not found in history")
        raise gr.Error("Could not find the selected video file.")
    
    video_path = video_item['local_filepath']
    print(f"üìÇ Video path: {video_path}")
    
    if not os.path.exists(video_path):
        print("‚ùå Video file not found")
        raise gr.Error(f"Video file not found at path: {video_path}")
    
    progress(0.1, desc="‚úÖ Video selected. Preparing for transcription...")
    
    try:
        # Extract audio
        audio_output_path = os.path.join(OUTPUT_FOLDER, f"audio_{os.path.basename(video_path)}.wav")
        print(f"üîä Extracting audio to: {audio_output_path}")
        progress(0.2, desc="üîä Extracting audio...")
        command = ["ffmpeg", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", str(SAMPLE_RATE), "-ac", "1", "-y", audio_output_path]
        subprocess.run(command, check=True, capture_output=True, text=True)
        
        # Load Whisper model and transcribe
        progress(0.4, desc="ü§ñ Loading AI model...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ü§ñ Loading Whisper model: {whisper_model_size} on device: {device}")
        model = whisper.load_model(whisper_model_size, device=device)
        
        progress(0.6, desc="‚úçÔ∏è Transcribing...")
        print("‚úçÔ∏è Starting transcription...")
        result = model.transcribe(audio_output_path, word_timestamps=True)
        
        print(f"üìù Transcription complete! Found {len(result['segments'])} segments")
        progress(0.9, desc="üìù Formatting transcript...")
        
        # Store transcript data globally for other functions to use
        global current_transcript_data
        current_transcript_data = {
            'segments': result["segments"],
            'video_title': selected_title,
            'video_path': video_path
        }
        print("üíæ Stored transcript data globally")
        
        # Build interactive HTML transcript
        formatted_transcript_html = build_interactive_transcript(result["segments"])
        print("üéØ Built interactive transcript HTML")
        
        # Clean up temporary audio file
        os.remove(audio_output_path)
        progress(1.0, desc="Transcription Complete!")
        print("‚úÖ Transcription process completed successfully!")
        
        # Enable subtitle export and search functionality
        return (
            video_path,  # vision_lab_video_player
            formatted_transcript_html,  # transcript_output
            gr.update(interactive=True),  # export_srt_button
            gr.update(interactive=True),  # export_vtt_button
            gr.update(interactive=True),  # search_input
            gr.update(interactive=True),  # search_button
            gr.update(value=""),  # search_results
            gr.update(interactive=True),   # create_clip_button
            gr.update(value="")  # debug_selection
        )
        
    except FileNotFoundError:
        print("‚ùå FFmpeg not found")
        raise gr.Error("ffmpeg not found. Please ensure ffmpeg is installed and in your system's PATH.")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå FFmpeg error: {e.stderr}")
        raise gr.Error(f"Error during audio extraction with ffmpeg: {e.stderr}")
    except Exception as e:
        print(f"‚ùå Unexpected error: {str(e)}")
        raise gr.Error(f"An unexpected error occurred during transcription: {str(e)}")

def build_interactive_transcript(segments):
    """Build an interactive HTML transcript with enhanced features"""
    
    # First, let's create a simple test to see if JavaScript works AT ALL
    html_content = f"""
    <div style="padding: 20px; background: #f0f8ff; border-radius: 10px; margin-bottom: 20px;">
        <h3 style="color: #2c3e50;">üß™ JavaScript Test</h3>
        <button onclick="alert('JavaScript is working!'); console.log('‚úÖ JS Test successful!');" 
                style="padding: 8px 16px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer;">
            Click to Test JavaScript
        </button>
        <p style="margin: 10px 0; font-size: 14px; color: #666;">
            If this button shows an alert, JavaScript is working. If not, there's a deeper issue.
        </p>
    </div>
    
    <div id="transcript-container" style="max-height: 500px; overflow-y: auto; padding: 15px; background: #f8f9fa; border-radius: 10px; border: 1px solid #dee2e6;">
        <style>
            .transcript-segment {{
                margin-bottom: 12px;
                padding: 12px 15px;
                background: white;
                border-radius: 8px;
                border-left: 4px solid #007bff;
                transition: all 0.2s ease;
                cursor: pointer;
                user-select: text;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }}
            .transcript-segment:hover {{
                background: #e3f2fd;
                transform: translateX(2px);
                box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            }}
            .transcript-segment.selected {{
                background: #bbdefb !important;
                border-left-color: #1976d2 !important;
                box-shadow: 0 4px 16px rgba(25,118,210,0.3) !important;
            }}
            .timestamp-link {{
                display: inline-block;
                background: #007bff;
                color: white !important;
                padding: 4px 10px;
                border-radius: 12px;
                text-decoration: none;
                font-size: 0.85em;
                font-weight: bold;
                margin-right: 12px;
                transition: background 0.2s;
                border: none;
            }}
            .timestamp-link:hover {{
                background: #0056b3 !important;
                text-decoration: none;
                color: white !important;
            }}
            .segment-text {{
                display: inline;
                line-height: 1.6;
                color: #2c3e50 !important;
                font-size: 14px;
                font-weight: 400;
            }}
        </style>
        <h4 style="color: #2c3e50; margin-bottom: 15px;">üìù Interactive Transcript</h4>
        <div id="selection-status" style="padding: 8px; background: #e8f5e8; border-radius: 5px; margin-bottom: 10px; font-size: 12px; color: #2e7d32;">
            Selected: <span id="selected-count">None</span>
        </div>
    """
    
    # Add transcript segments with simpler inline JavaScript
    for i, segment in enumerate(segments):
        start_seconds = segment['start']
        end_seconds = segment['end']
        start_time_str = time.strftime('%M:%S', time.gmtime(start_seconds))
        
        # Clean text
        segment_text = segment['text'].strip().replace('<', '&lt;').replace('>', '&gt;')
        
        # Use simple inline JavaScript with try-catch
        html_content += f"""
        <div class="transcript-segment" 
             data-start="{start_seconds}" 
             data-index="{i}"
             onclick="
                try {{
                    console.log('üéØ Segment {i} clicked - time: {start_seconds}');
                    
                    // Seek video - try multiple selectors
                    let video = document.querySelector('#vision_lab_player video') || 
                               document.querySelector('video') ||
                               document.querySelectorAll('video')[0];
                    
                    if (video) {{
                        video.currentTime = {start_seconds};
                        console.log('‚úÖ Video seeked to: {start_seconds}');
                    }} else {{
                        console.log('‚ùå No video found');
                        console.log('Available videos:', document.querySelectorAll('video').length);
                    }}
                    
                    // Handle selection
                    if (event.ctrlKey || event.metaKey) {{
                        this.classList.toggle('selected');
                        console.log('üîÄ Multi-select toggled');
                    }} else {{
                        document.querySelectorAll('.transcript-segment.selected').forEach(el => el.classList.remove('selected'));
                        this.classList.add('selected');
                        console.log('üéØ Single selected');
                    }}
                    
                    // Update count display
                    let selectedCount = document.querySelectorAll('.transcript-segment.selected').length;
                    let countEl = document.getElementById('selected-count');
                    if (countEl) countEl.textContent = selectedCount || 'None';
                    
                    // Update hidden input
                    let selectedIndices = Array.from(document.querySelectorAll('.transcript-segment.selected'))
                        .map(el => el.dataset.index).join(',');
                    console.log('üìã Selected indices:', selectedIndices);
                    
                    // Try to find and update hidden input
                    let hiddenInput = document.querySelector('#selected_segments_input input') ||
                                     document.querySelector('#selected_segments_input textarea') ||
                                     Array.from(document.querySelectorAll('input, textarea')).find(inp => 
                                         inp.style.display === 'none' || inp.type === 'hidden');
                    
                    if (hiddenInput) {{
                        hiddenInput.value = selectedIndices;
                        hiddenInput.dispatchEvent(new Event('input', {{bubbles: true}}));
                        console.log('üíæ Updated hidden input:', selectedIndices);
                    }} else {{
                        console.log('‚ùå Hidden input not found');
                    }}
                    
                }} catch (error) {{
                    console.error('‚ùå Error in click handler:', error);
                }}
                return false;
             ">
            <span class="timestamp-link">{start_time_str}</span>
            <span class="segment-text">{segment_text}</span>
        </div>
        """
    
    html_content += """
    </div>
    
    <script>
        // Simple verification that script tags work
        console.log("üé¨ Transcript script loaded - timestamp:", new Date().toISOString());
        console.log("üéØ Ready for interaction!");
    </script>
    """
    
    return html_content

def export_subtitles(format_type):
    """Export subtitles in SRT or VTT format"""
    global current_transcript_data
    
    if 'current_transcript_data' not in globals() or not current_transcript_data:
        raise gr.Error("No transcript data available. Please transcribe a video first.")
    
    segments = current_transcript_data['segments']
    video_title = current_transcript_data['video_title']
    
    # Clean title for filename
    safe_title = safe_filename(video_title)
    
    if format_type == 'srt':
        content = generate_srt_content(segments)
        filename = f"{safe_title}.srt"
    else:  # vtt
        content = generate_vtt_content(segments)
        filename = f"{safe_title}.vtt"
    
    filepath = os.path.join(OUTPUT_FOLDER, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return gr.File(value=filepath, visible=True)

def search_in_transcript(search_query):
    """Search for text in the current transcript"""
    global current_transcript_data
    
    if 'current_transcript_data' not in globals() or not current_transcript_data:
        return "No transcript available. Please transcribe a video first."
    
    if not search_query or not search_query.strip():
        return "Please enter a search term."
    
    segments = current_transcript_data['segments']
    results = search_transcript(segments, search_query)
    
    if not results:
        return f"No results found for '{search_query}'."
    
    # Build search results HTML
    results_html = f"<div style='max-height: 300px; overflow-y: auto;'><h4>Found {len(results)} result(s) for '{search_query}':</h4>"
    
    for result in results:
        start_time_str = time.strftime('%M:%S', time.gmtime(result['start_time']))
        results_html += f"""
        <div style='margin: 10px 0; padding: 10px; background: #f0f8ff; border-radius: 5px; border-left: 3px solid #007bff;'>
            <a href="#" onclick="seekVideoAndHighlight(document.querySelector('[data-index=\"{result['segment_index']}\"]'), {result['start_time']}); return false;" 
               style='color: #007bff; font-weight: bold; text-decoration: none;'>
                [{start_time_str}]
            </a>
            <p style='margin: 5px 0 0 0;'>{result['highlighted_text']}</p>
        </div>
        """
    
    results_html += "</div>"
    return results_html

def create_video_clip(selected_segments_str):
    """Create a video clip from selected transcript segments"""
    print("üé¨ Starting create_video_clip function")
    print(f"üìù Selected segments string: '{selected_segments_str}'")
    
    global current_transcript_data
    
    if 'current_transcript_data' not in globals() or not current_transcript_data:
        print("‚ùå No transcript data available")
        raise gr.Error("No transcript data available. Please transcribe a video first.")
    
    if not selected_segments_str or not selected_segments_str.strip():
        print("‚ùå No segments selected - empty string received")
        print("üîç Debug info:")
        print(f"  - selected_segments_str type: {type(selected_segments_str)}")
        print(f"  - selected_segments_str repr: {repr(selected_segments_str)}")
        raise gr.Error("No segments selected. Click on transcript segments while holding Ctrl/Cmd to select multiple segments.")
    
    try:
        # Parse selected segment indices
        print(f"üî¢ Parsing segment indices from: {selected_segments_str}")
        segment_indices = [int(i.strip()) for i in selected_segments_str.split(',') if i.strip()]
        print(f"üìã Parsed segment indices: {segment_indices}")
        
        if not segment_indices:
            print("‚ùå No valid segment indices found")
            raise gr.Error("No valid segments selected.")
        
        segments = current_transcript_data['segments']
        video_path = current_transcript_data['video_path']
        video_title = current_transcript_data['video_title']
        
        print(f"üìπ Video path: {video_path}")
        print(f"üè∑Ô∏è Video title: {video_title}")
        print(f"üìä Total segments available: {len(segments)}")
        
        # Sort indices and get start/end times
        segment_indices.sort()
        start_time = segments[segment_indices[0]]['start']
        end_time = segments[segment_indices[-1]]['end']
        
        print(f"‚è∞ Clip time range: {start_time:.2f}s to {end_time:.2f}s")
        
        # Create safe filename
        safe_title = safe_filename(video_title)
        clip_filename = f"{safe_title}_clip_{int(start_time)}s-{int(end_time)}s.mp4"
        clip_path = os.path.join(CLIPS_FOLDER, clip_filename)
        
        print(f"üíæ Clip will be saved to: {clip_path}")
        
        # Create clip using GPU-accelerated FFmpeg
        command = ["ffmpeg"]
        
        # Add GPU decode acceleration if available
        decode_params = get_optimized_ffmpeg_params('decode')
        if decode_params:
            command.extend(decode_params)
        
        command.extend([
            "-i", video_path,
            "-ss", str(start_time),
            "-to", str(end_time),
            "-c", "copy",  # Copy streams for speed since we're just trimming
            "-y", clip_path
        ])
        
        print(f"üöÄ FFmpeg command with GPU acceleration: {' '.join(command[:6])}...")
        subprocess.run(command, check=True, capture_output=True, text=True)
        
        if os.path.exists(clip_path):
            duration = end_time - start_time
            print(f"‚úÖ Clip created successfully! Duration: {duration:.1f} seconds")
            return (
                gr.File(value=clip_path, visible=True),
                f"‚úÖ Clip created successfully! Duration: {duration:.1f} seconds"
            )
        else:
            print("‚ùå Clip file was not created")
            raise gr.Error("Failed to create clip file.")
            
    except subprocess.CalledProcessError as e:
        print(f"‚ùå FFmpeg error: {e.stderr}")
        raise gr.Error(f"FFmpeg error while creating clip: {e.stderr}")
    except Exception as e:
        print(f"‚ùå Unexpected error: {str(e)}")
        raise gr.Error(f"Error creating clip: {str(e)}")
    
# --- Enhanced Detector Class ---
class EnhancedDetector:
    def __init__(self):
        self.model = None
        self.video_path = None
        self.cap = None
        self.detection_active = False
        self.current_detections = []
        self.video_fps = 0

    def load_model(self, model_path):
        try:
            self.model = YOLO(model_path)
            print(f"‚úÖ YOLO model loaded: {model_path}")
            return True
        except Exception as e:
            print(f"‚ùå Failed to load YOLO: {str(e)}")
            return False

    def open_video(self, video_path):
        try:
            self.cap = cv2.VideoCapture(video_path)
            if not self.cap.isOpened():
                raise ValueError("Could not open video")
            self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)
            self.video_path = video_path
            print(f"‚úÖ Video opened: {video_path} ({self.video_fps} FPS)")
            return True
        except Exception as e:
            print(f"‚ùå Failed to open video: {str(e)}")
            return False

    def get_frame_at_timestamp(self, timestamp_str, conf_threshold=0.5):
        if not self.model or not self.cap:
            return None

        try:
            timestamp = float(timestamp_str)
            frame_pos = int(timestamp * self.video_fps)
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)
            ret, frame = self.cap.read()

            if not ret:
                return None

            # Perform detection
            results = self.model(frame, conf=conf_threshold)
            annotated_frame = results[0].plot()  # Draw bounding boxes

            # Convert to RGB for Gradio
            annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
            return annotated_frame

        except Exception as e:
            print(f"‚ùå Detection error: {str(e)}")
            return None

    def start_realtime_detection(self, video_path, conf_threshold):
        if self.open_video(video_path):
            self.detection_active = True
            return True
        return False

    def stop_detection(self):
        self.detection_active = False
        if self.cap:
            self.cap.release()

    def test_detection(self):
        if not self.model:
            return "‚ùå No model loaded"
        try:
            # Test with a blank image
            test_img = np.zeros((640, 640, 3), dtype=np.uint8)
            results = self.model(test_img)
            return f"‚úÖ Model test successful! Detected {len(results[0].boxes)} objects (expected 0 on blank image)"
        except Exception as e:
            return f"‚ùå Model test failed: {str(e)}"

# Instantiate the detector globally (THIS IS THE FIX - ensure it's here before gr.Blocks())
enhanced_detector = EnhancedDetector()    

# --- CSS Styling ---
custom_css = """
.gradio-container {max-width: 1400px; margin: auto;}
.preview-card { background: linear-gradient(145deg, #f0f2f5, #ffffff); border-radius: 15px; padding: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1); border: 1px solid #e1e5e9; }
.main-title { color: white; font-size: 2.5em; font-weight: bold; text-align: center; margin-bottom: 10px; }  # Updated: Solid white color
.subtitle { text-align: center; color: #666; font-size: 1.2em; margin-bottom: 30px; }
.section-header { background: linear-gradient(90deg, #667eea, #764ba2); color: white; padding: 10px 15px; border-radius: 8px; font-weight: bold; margin-bottom: 15px; text-align: center; }
.feature-card { background: #223552; border: 2px solid #dee2e6; border-radius: 12px; padding: 20px; margin: 10px 0; box-shadow: 0 2px 10px rgba(0,0,0,0.08); }
.feature-card h3 { color: #ffffff; font-weight: bold; margin-bottom: 15px; padding-bottom: 8px; border-bottom: 2px solid #3498db; }
.feature-card h4 { color: #e8f4fd; font-weight: 600; margin-top: 20px; margin-bottom: 10px; }
.step-title { background: linear-gradient(135deg, #667eea, #764ba2); color: white !important; padding: 8px 15px; border-radius: 8px; font-weight: bold; margin-bottom: 10px; display: inline-block; }
"""

# JavaScript to link video time to the hidden textbox
js_code = """
function() {
    console.log("üé¨ Setting up FAST video timestamp tracker...");
    
    setTimeout(() => {
        const video = document.querySelector("#vision_video_player video");
        const timestampInput = document.querySelector("#video_timestamp_state input, #video_timestamp_state textarea");
        
        if (!video || !timestampInput) {
            console.log("‚ùå Elements not found, retrying in 1 second...");
            setTimeout(arguments.callee, 1000);
            return;
        }
        
        console.log("‚úÖ Found video and timestamp input elements");
        
        let lastUpdate = 0;
        let lastTime = 0;
        
        function updateTimestamp() {
            const now = Date.now();
            // SPEED FIX: Update every 200ms instead of 500ms for faster response
            if (now - lastUpdate < 200) return;
            
            if (!video.paused && !video.ended && video.currentTime > 0) {
                // SPEED FIX: Only update if time actually changed
                if (Math.abs(video.currentTime - lastTime) > 0.1) {
                    const newValue = video.currentTime.toFixed(1);
                    timestampInput.value = newValue;
                    timestampInput.dispatchEvent(new Event('input', { bubbles: true }));
                    lastUpdate = now;
                    lastTime = video.currentTime;
                    console.log(`‚ö° Fast video time: ${newValue}s`);
                }
            }
        }
        
        // SPEED FIX: Use requestAnimationFrame for smoother updates
        function animationLoop() {
            updateTimestamp();
            requestAnimationFrame(animationLoop);
        }
        
        // Start the animation loop
        requestAnimationFrame(animationLoop);
        
        // Backup: also use timeupdate event
        video.addEventListener("timeupdate", updateTimestamp);
        
        console.log("‚ö° FAST timestamp tracker active with optimized detection!");
        
    }, 500);
    
    return "üöÄ Fast video tracking initialized";
}
"""

print("üîß Apply these changes to fix YOLO11 detection!")
print("‚úÖ Key fixes:")
print("  1. Proper tensor handling with .cpu().numpy()")
print("  2. Lower confidence threshold (0.25 default)")
print("  3. Better debugging output")
print("  4. Model testing functionality")
print("  5. Improved error handling")

# --- Main Interface ---
with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title="StreamSnap") as demo:
    # Global state
    download_history_state = gr.State(value=load_history(DOWNLOAD_HISTORY_FILE))
    current_transcript_data = {}
    current_vision_analysis = {}
    
    # Initialize GPU acceleration detection
    print("üöÄ Initializing StreamSnap with GPU acceleration...")
    gpu_type, gpu_config = detect_gpu_acceleration()
    if gpu_type:
        print(f"‚úÖ GPU acceleration enabled: {gpu_config['name']}")
    else:
        print("‚ö†Ô∏è Using CPU processing (GPU acceleration not available)")
    
    # Header
    gpu_status = f" ‚Ä¢ {gpu_config['name']}" if gpu_config else ""
    gr.HTML(f'<div class="main-title">üì∫ StreamSnap</div><div class="subtitle">Download ‚Ä¢ Transcribe ‚Ä¢ Real-Time Object Detection{gpu_status}</div>')
    
    with gr.Tabs():
        # StreamDL Tab (YouTube Download)
        with gr.TabItem("üöÄ StreamDL", id="tab_streamdl"):
            with gr.Row():
                with gr.Column(scale=2):
                    with gr.Row():
                        url_input = gr.Textbox(label="üîó YouTube URL", placeholder="Paste any YouTube URL here...", scale=4)
                        preview_button = gr.Button("üîç Preview", scale=1, variant="secondary")
                    
                    with gr.Accordion("‚öôÔ∏è Download Settings", open=True):
                        audio_only_checkbox = gr.Checkbox(label="üéµ Audio Only Mode")
                        with gr.Row():
                            video_quality_dropdown = gr.Dropdown(["1080p", "720p", "480p", "360p"], value="720p", label="üì∫ Video Quality")
                            video_format_dropdown = gr.Dropdown(['mp4', 'mkv', 'webm'], value="mp4", label="üé¨ Video Format")
                        audio_format_dropdown = gr.Dropdown(['mp3', 'm4a', 'wav', 'flac'], value="mp3", label="üéµ Audio Format", visible=False)
                        use_gpu_checkbox = gr.Checkbox(label="üöÄ Use GPU Acceleration (Recommended)", value=True if gpu_type else False)
                    
                    download_button = gr.Button("‚¨áÔ∏è Download Now", variant="primary", interactive=False, size="lg")
                    result_output = gr.Textbox(label="üìä Status", interactive=False, value="Ready! Enter a URL to get started...")
                
                with gr.Column(scale=1):
                    with gr.Group(visible=False, elem_classes=["preview-card"]) as preview_group:
                        gr.HTML('<div class="section-header">üîç Preview</div>')
                        preview_thumbnail = gr.Image(label="Thumbnail", height=200)
                        preview_title = gr.Textbox(label="üìù Title", interactive=False)
                        with gr.Row():
                            preview_duration = gr.Textbox(label="‚è±Ô∏è Duration", interactive=False, scale=1)
                            preview_uploader = gr.Textbox(label="üë§ Channel", interactive=False, scale=2)
                        hidden_thumbnail_url = gr.Textbox(visible=False)
            
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column(scale=2):
                    gr.HTML('<div class="section-header">üéÆ Media Player</div>')
                    playback_video = gr.Video(label="Video Player", visible=False, height=400)
                    playback_audio = gr.Audio(label="Audio Player", visible=False)
                
                with gr.Column(scale=1):
                    gr.HTML('<div class="section-header">üìö Download History</div>')
                    download_history_slider = gr.Slider(
                        minimum=0, 
                        maximum=max(1, len(load_history(DOWNLOAD_HISTORY_FILE)) - 5) if len(load_history(DOWNLOAD_HISTORY_FILE)) > 6 else 1, 
                        step=1, value=0, label="Navigate History", interactive=True
                    )
                    download_history_info = gr.Textbox(
                        value="Showing recent downloads" if load_history(DOWNLOAD_HISTORY_FILE) else "No downloads yet", 
                        label="üìä Info", interactive=False
                    )
                    download_history_gallery = gr.Gallery(
                        label="Click to play", 
                        value=[(item.get('thumbnail_url', ''), f"{item['title'][:50]}...") for item in load_history(DOWNLOAD_HISTORY_FILE)[:6]], 
                        columns=1, object_fit="contain", height=350, show_label=False
                    )

        # Intelligent Scribe Tab (Transcription)
        with gr.TabItem("‚úçÔ∏è Intelligent Scribe", id="tab_intelligent_scribe"):
            gr.HTML('<div class="section-header">üß† Intelligent Scribe Pro</div>')
            gr.Markdown("Transform your videos into interactive, searchable transcripts with advanced features.")
            
            with gr.Row():
                # Left Column - Controls
                with gr.Column(scale=1, elem_classes=["feature-card"]):
                    gr.HTML('<div class="step-title">üéØ Step 1: Select Video</div>')
                    video_history = [item for item in load_history(DOWNLOAD_HISTORY_FILE) if item['media_type'] == 'video']
                    video_choices = [f"[{item['media_type']}] {item['title']}" for item in video_history]
                    video_selector = gr.Dropdown(
                        choices=video_choices if video_choices else ["No videos available"], 
                        label="Select Downloaded Video", 
                        value=video_choices[0] if video_choices else "No videos available"
                    )
                    
                    gr.HTML('<div class="step-title">ü§ñ Step 2: AI Model</div>')
                    whisper_model_selector = gr.Dropdown(
                        choices=["tiny", "base", "small", "medium", "large"], 
                        value="base", 
                        label="Whisper Model", 
                        info="Larger = more accurate but slower"
                    )
                    
                    gr.HTML('<div class="step-title">üöÄ Step 3: Generate</div>')
                    transcribe_button = gr.Button("‚úçÔ∏è Transcribe Video", variant="primary", size="lg")
                    
                    gr.Markdown("---")
                    
                    # Export Features
                    gr.HTML('<div class="step-title">üì§ Export Subtitles</div>')
                    with gr.Row():
                        export_srt_button = gr.Button("üìÑ Export SRT", interactive=False)
                        export_vtt_button = gr.Button("üåê Export VTT", interactive=False)
                    
                    subtitle_file_output = gr.File(label="Download Subtitle File", visible=False)
                    
                    gr.HTML('<div class="step-title">üîç Search Transcript</div>')
                    with gr.Row():
                        search_input = gr.Textbox(placeholder="Search for words or phrases...", scale=3, interactive=False)
                        search_button = gr.Button("üîç", scale=1, interactive=False)
                    
                    search_results = gr.HTML(value="")
                    
                    gr.HTML('<div class="step-title">‚úÇÔ∏è Create Clips</div>')
                    gr.Markdown("*Select transcript segments (Ctrl+Click for multiple) then create clips*")
                    selected_segments_input = gr.Textbox(visible=False, elem_id="selected_segments_input")
                    
                    # Add a visible debug textbox to see what's selected
                    debug_selection = gr.Textbox(label="üîç Debug: Selected Segments", value="", interactive=False, visible=True)
                    create_clip_button = gr.Button("üé¨ Create Clip from Selection", interactive=False)
                    clip_status = gr.Textbox(label="Clip Status", value="", visible=False)
                    clip_file_output = gr.File(label="Download Clip", visible=False)
                
                # Right Column - Video and Transcript
                with gr.Column(scale=2):
                    gr.HTML('<div class="step-title">üé• Video Player</div>')
                    vision_lab_video_player = gr.Video(label="Selected Video", height=350, elem_id="vision_lab_player")
                    
                    gr.HTML('<div class="step-title">üìù Interactive Transcript</div>')
                    gr.Markdown("*Click timestamps to jump to moments ‚Ä¢ Ctrl+Click segments to select for clipping*")
                    transcript_output = gr.HTML(value="<div style='padding: 40px; text-align: center; color: #666; background: #f8f9fa; border-radius: 10px; border: 1px solid #dee2e6;'><h4 style='color: #2c3e50;'>üé¨ Your interactive transcript will appear here</h4><p>After transcription, you'll be able to click timestamps to navigate and select segments for clipping!</p></div>")

        # VISION LAB - DIRECT-TO-DETECTION
        with gr.TabItem("üî¨ Vision Lab", id="tab_vision_lab"):
            gr.HTML('<div class="section-header">ü§ñ Live AI Object Detection</div>')
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.HTML('<div class="step-title">‚öôÔ∏è Settings</div>')
                    
                    vision_video_selector = gr.Dropdown(
                        choices=[f"[{item['media_type']}] {item['title']}" for item in load_history(DOWNLOAD_HISTORY_FILE) if item['media_type'] == 'video'] or ["No videos available"],
                        label="Select Video"
                    )

                    model_size_selector = gr.Dropdown(
                        choices=[
                            ("Nano - Fastest", "yolo11n.pt"),
                            ("Small - Balanced", "yolo11s.pt"), 
                            ("Medium - Accurate", "yolo11m.pt"),
                            ("Large - High Accuracy", "yolo11l.pt"),
                            ("Extra Large - Maximum Accuracy", "yolo11x.pt")
                        ],
                        value="yolo11s.pt",
                        label="üéØ Model Size",
                        info="Larger models = more accurate but slower processing"
                    )

                    # LOWER CONFIDENCE THRESHOLD
                    confidence_threshold = gr.Slider(
                        minimum=0.1, maximum=0.8, value=0.25,  # CHANGED: Lower default
                        step=0.05,
                        label="Confidence Threshold",
                        info="Lower = more detections"
                    )

                    # Keep the smart confidence boost as a simple toggle
                    confidence_boost = gr.Checkbox(
                        label="üß† Smart Detection Filter",
                        value=True,
                        info="Automatically filters out false positives and improves accuracy"
                    )

                    # Add minimum confidence per object type
                    confidence_boost = gr.Checkbox(
                        label="üß† Smart Confidence Boost",
                        value=True,
                        info="Automatically require higher confidence for commonly misclassified objects"
                    )

                    gr.Markdown("""
                    **üéØ Auto-Detection Info:**
                    - **Computers**: Laptops, monitors (as TV), keyboards, mice, phones
                    - **Furniture**: Chairs, tables/desks, couches, beds  
                    - **Electronics**: TVs/monitors, remotes, clocks, phones
                    - **Office Items**: Books/documents, keyboards, mice, laptops
                    - **People & Vehicles**: People, cars, trucks, bicycles, motorcycles
                    - **Animals**: Cats, dogs, birds, horses, and more
                    - **Common Objects**: Bottles, cups, plants, and 80+ other items

                    *Smart filtering automatically adjusts confidence levels for better accuracy.*
                    """)

                    start_detection_button = gr.Button("üöÄ Start Detection", variant="primary")
                    
                    # ADD TEST BUTTON
                    test_button = gr.Button("üß™ Test YOLO", variant="secondary")
                    
                    analysis_status = gr.Textbox(label="Status", value="Ready", interactive=False)

                with gr.Column(scale=2):
                    # Hidden timestamp tracker
                    video_timestamp_state = gr.Textbox("0.0", visible=False, elem_id="video_timestamp_state")

                    # Video player
                    vision_video_player = gr.Video(
                        label="Video Player",
                        height=400,
                        elem_id="vision_video_player"
                    )
                    
                    # Live detection overlay
                    live_overlay_image = gr.Image(
                        label="ü§ñ Live Detection Overlay",
                        height=400,
                        visible=False
                    )

    # --- Event Handlers ---
    
    # StreamSnap events
    preview_button.click(
        fn=get_video_info, 
        inputs=url_input, 
        outputs=[preview_group, preview_thumbnail, preview_title, preview_duration, preview_uploader, hidden_thumbnail_url, download_button, result_output]
    )
    
    audio_only_checkbox.change(
        fn=lambda is_audio: {
            video_quality_dropdown: gr.update(visible=not is_audio),
            video_format_dropdown: gr.update(visible=not is_audio),
            audio_format_dropdown: gr.update(visible=is_audio)
        },
        inputs=audio_only_checkbox,
        outputs=[video_quality_dropdown, video_format_dropdown, audio_format_dropdown]
    )
    
    download_button.click(
        fn=download_video_or_audio,
        inputs=[url_input, audio_only_checkbox, video_quality_dropdown, video_format_dropdown, audio_format_dropdown, hidden_thumbnail_url, use_gpu_checkbox],
        outputs=[result_output, playback_video, playback_audio, download_history_gallery, download_history_info, download_history_slider, download_history_state, video_selector, vision_video_selector]
    )
    
    download_history_slider.change(
        fn=update_history_display,
        inputs=[download_history_state, download_history_slider],
        outputs=[download_history_gallery, download_history_info]
    )
    
    download_history_gallery.select(
        fn=play_from_download_history,
        inputs=[download_history_state, download_history_slider],
        outputs=[playback_video, playback_audio]
    )
    
    # Vision Lab events
    transcribe_button.click(
        fn=transcribe_video,
        inputs=[video_selector, download_history_state, whisper_model_selector],
        outputs=[
            vision_lab_video_player, 
            transcript_output, 
            export_srt_button, 
            export_vtt_button, 
            search_input, 
            search_button,
            search_results,
            create_clip_button,
            debug_selection
        ]
    )
    
    # Export subtitle events
    export_srt_button.click(
        fn=lambda: export_subtitles('srt'),
        outputs=subtitle_file_output
    )
    
    export_vtt_button.click(
        fn=lambda: export_subtitles('vtt'),
        outputs=subtitle_file_output
    )
    
    # Search events
    search_button.click(
        fn=search_in_transcript,
        inputs=search_input,
        outputs=search_results
    )
    
    search_input.submit(
        fn=search_in_transcript,
        inputs=search_input,
        outputs=search_results
    )
    
    # Clip creation event
    create_clip_button.click(
        fn=create_video_clip,
        inputs=[selected_segments_input],
        outputs=[clip_file_output, clip_status]
    )

    # Update smart confidence when toggle changes
    confidence_boost.change(
        fn=lambda smart: setattr(enhanced_detector, 'use_smart_confidence', smart) if enhanced_detector else None,
        inputs=[confidence_boost],
        outputs=[]
    )
    
    # --- VISION LAB FUNCTIONS (define inside the blocks context) ---
    
    def setup_live_detection(video_path_str, history_data, model_size):
        """FIXED: Setup with proper error handling and testing"""
        print("üöÄ Setting up live detection...")
        
        if not video_path_str or video_path_str == "No videos available":
            raise gr.Error("Please select a video first!")
        
        # Load model first
        print(f"ü§ñ Loading YOLO model: {model_size}")
        success = enhanced_detector.load_model(model_size)
        if not success:
            raise gr.Error("Failed to load YOLO model!")
        
        # Test model immediately
        test_result = enhanced_detector.test_detection()
        print(f"üß™ Model test result: {test_result}")
        
        # Get video path
        selected_title = video_path_str.split("] ", 1)[1]
        video_item = next((item for item in history_data if item['title'] == selected_title), None)
        if not video_item:
            raise gr.Error("Video not found in history!")
        
        video_path = video_item['local_filepath']
        print(f"üìÇ Video path: {video_path}")
        
        if not os.path.exists(video_path):
            raise gr.Error(f"Video file not found: {video_path}")
        
        # Open video
        success = enhanced_detector.open_video(video_path)
        if not success:
            raise gr.Error("Failed to open video file!")
        
        print("‚úÖ Live detection setup complete!")
        
        return {
            vision_video_player: gr.update(value=video_path),
            live_overlay_image: gr.update(visible=True, label="ü§ñ Live Detection Feed - Play video to see detection!"),
            analysis_status: gr.update(value="‚úÖ Ready! Play the video to see real-time object detection with bounding boxes!")
        }
        
    # --- VISION LAB EVENT HANDLERS (MOVE THESE INSIDE!) ---
    
    # Start detection button
    start_detection_button.click(
        fn=setup_live_detection,
        inputs=[vision_video_selector, download_history_state, model_size_selector],
        outputs=[vision_video_player, live_overlay_image, analysis_status]
    )
    
    # Test button handler
    test_button.click(
        fn=lambda: enhanced_detector.test_detection() if enhanced_detector.model else "No model loaded",
        outputs=analysis_status
    )
    
    # CRITICAL: Enhanced timestamp change handler with better error handling
    video_timestamp_state.change(
        fn=lambda timestamp, confidence: enhanced_detector.get_frame_at_timestamp(timestamp, confidence) if enhanced_detector and enhanced_detector.model and enhanced_detector.cap else None,
        inputs=[video_timestamp_state, confidence_threshold],
        outputs=[live_overlay_image],
        show_progress="hidden"
    )

    # Add video loading event to trigger JavaScript
    vision_video_player.change(
        fn=None,
        js=js_code,
        inputs=[],
        outputs=[]
    )

# NOW the demo.launch() should be OUTSIDE the blocks context
if __name__ == "__main__":
    demo.launch(share=True, debug=True)

print("üîß YOLO11 detection fixes applied!")
print("‚úÖ Fixed: Proper YOLO11 syntax, lower confidence thresholds, model testing")
print("üéØ Try confidence 0.3-0.5 for better detection results")
print("üß™ Use 'Test YOLO11 Detection' button to verify model works")